{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9390d3a2-e1cc-4c1f-bb42-c33d99ded218",
   "metadata": {},
   "source": [
    "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd548299-8620-4a8d-a733-c571230b1660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.12/site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4) (4.14.0)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.12/site-packages (from konlpy) (2.2.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Get:1 http://archive.ubuntu.com/ubuntu noble InRelease [256 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Packages [33.1 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu noble/universe amd64 Packages [19.3 MB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu noble-security/main amd64 Packages [1643 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Packages [1170 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Packages [2709 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu noble/main amd64 Packages [1808 kB]    \n",
      "Get:11 http://archive.ubuntu.com/ubuntu noble/restricted amd64 Packages [117 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu noble/multiverse amd64 Packages [331 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1939 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Packages [35.9 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [2007 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Packages [2847 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Packages [49.4 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Packages [33.9 kB]\n",
      "Fetched 34.7 MB in 5s (6694 kB/s)               \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  ca-certificates-java java-common libnspr4 libnss3 libpcsclite1\n",
      "  openjdk-11-jre-headless\n",
      "Suggested packages:\n",
      "  default-jre pcscd openjdk-11-demo openjdk-11-source libnss-mdns\n",
      "  fonts-dejavu-extra fonts-ipafont-gothic fonts-ipafont-mincho\n",
      "  fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  ca-certificates-java java-common libnspr4 libnss3 libpcsclite1\n",
      "  openjdk-11-jdk-headless openjdk-11-jre-headless\n",
      "0 upgraded, 7 newly installed, 0 to remove and 72 not upgraded.\n",
      "Need to get 118 MB of archives.\n",
      "After this operation, 263 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu noble/main amd64 ca-certificates-java all 20240118 [11.6 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu noble/main amd64 java-common all 0.75+exp1 [6798 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu noble/main amd64 libnspr4 amd64 2:4.35-1.1build1 [117 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu noble/main amd64 libnss3 amd64 2:3.98-1build1 [1445 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu noble/main amd64 libpcsclite1 amd64 2.0.3-1build1 [21.4 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~24.04.1 [42.3 MB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~24.04.1 [73.7 MB]\n",
      "Fetched 118 MB in 8s (14.7 MB/s)                                               \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package ca-certificates-java.\n",
      "(Reading database ... 52749 files and directories currently installed.)\n",
      "Preparing to unpack .../0-ca-certificates-java_20240118_all.deb ...\n",
      "Unpacking ca-certificates-java (20240118) ...\n",
      "Selecting previously unselected package java-common.\n",
      "Preparing to unpack .../1-java-common_0.75+exp1_all.deb ...\n",
      "Unpacking java-common (0.75+exp1) ...\n",
      "Selecting previously unselected package libnspr4:amd64.\n",
      "Preparing to unpack .../2-libnspr4_2%3a4.35-1.1build1_amd64.deb ...\n",
      "Unpacking libnspr4:amd64 (2:4.35-1.1build1) ...\n",
      "Selecting previously unselected package libnss3:amd64.\n",
      "Preparing to unpack .../3-libnss3_2%3a3.98-1build1_amd64.deb ...\n",
      "Unpacking libnss3:amd64 (2:3.98-1build1) ...\n",
      "Selecting previously unselected package libpcsclite1:amd64.\n",
      "Preparing to unpack .../4-libpcsclite1_2.0.3-1build1_amd64.deb ...\n",
      "Unpacking libpcsclite1:amd64 (2.0.3-1build1) ...\n",
      "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
      "Preparing to unpack .../5-openjdk-11-jre-headless_11.0.28+6-1ubuntu1~24.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~24.04.1) ...\n",
      "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
      "Preparing to unpack .../6-openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~24.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~24.04.1) ...\n",
      "Setting up java-common (0.75+exp1) ...\n",
      "Setting up libnspr4:amd64 (2:4.35-1.1build1) ...\n",
      "Setting up libpcsclite1:amd64 (2.0.3-1build1) ...\n",
      "Setting up ca-certificates-java (20240118) ...\n",
      "No JRE found. Skipping Java certificates setup.\n",
      "Setting up libnss3:amd64 (2:3.98-1build1) ...\n",
      "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~24.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
      "Processing triggers for libc-bin (2.39-0ubuntu8.4) ...\n",
      "Processing triggers for ca-certificates-java (20240118) ...\n",
      "Adding debian:ACCVRAIZ1.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
      "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
      "Adding debian:Actalis_Authentication_Root_CA.pem\n",
      "Adding debian:AffirmTrust_Commercial.pem\n",
      "Adding debian:AffirmTrust_Networking.pem\n",
      "Adding debian:AffirmTrust_Premium.pem\n",
      "Adding debian:AffirmTrust_Premium_ECC.pem\n",
      "Adding debian:Amazon_Root_CA_1.pem\n",
      "Adding debian:Amazon_Root_CA_2.pem\n",
      "Adding debian:Amazon_Root_CA_3.pem\n",
      "Adding debian:Amazon_Root_CA_4.pem\n",
      "Adding debian:Atos_TrustedRoot_2011.pem\n",
      "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
      "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
      "Adding debian:BJCA_Global_Root_CA1.pem\n",
      "Adding debian:BJCA_Global_Root_CA2.pem\n",
      "Adding debian:Baltimore_CyberTrust_Root.pem\n",
      "Adding debian:Buypass_Class_2_Root_CA.pem\n",
      "Adding debian:Buypass_Class_3_Root_CA.pem\n",
      "Adding debian:CA_Disig_Root_R2.pem\n",
      "Adding debian:CFCA_EV_ROOT.pem\n",
      "Adding debian:COMODO_Certification_Authority.pem\n",
      "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
      "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
      "Adding debian:Certainly_Root_E1.pem\n",
      "Adding debian:Certainly_Root_R1.pem\n",
      "Adding debian:Certigna.pem\n",
      "Adding debian:Certigna_Root_CA.pem\n",
      "Adding debian:Certum_EC-384_CA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
      "Adding debian:Certum_Trusted_Root_CA.pem\n",
      "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
      "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
      "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
      "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
      "Adding debian:Comodo_AAA_Services_root.pem\n",
      "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
      "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
      "Adding debian:DigiCert_Global_Root_CA.pem\n",
      "Adding debian:DigiCert_Global_Root_G2.pem\n",
      "Adding debian:DigiCert_Global_Root_G3.pem\n",
      "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
      "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
      "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
      "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
      "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
      "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
      "Adding debian:GLOBALTRUST_2020.pem\n",
      "Adding debian:GTS_Root_R1.pem\n",
      "Adding debian:GTS_Root_R2.pem\n",
      "Adding debian:GTS_Root_R3.pem\n",
      "Adding debian:GTS_Root_R4.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
      "Adding debian:GlobalSign_Root_CA.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
      "Adding debian:GlobalSign_Root_E46.pem\n",
      "Adding debian:GlobalSign_Root_R46.pem\n",
      "Adding debian:Go_Daddy_Class_2_CA.pem\n",
      "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
      "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
      "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
      "Adding debian:ISRG_Root_X1.pem\n",
      "Adding debian:ISRG_Root_X2.pem\n",
      "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
      "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
      "Adding debian:Izenpe.com.pem\n",
      "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
      "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
      "Adding debian:NetLock_Arany_=Class_Gold=_FÅ‘tanÃºsÃ­tvÃ¡ny.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
      "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA_2.pem\n",
      "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
      "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
      "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
      "Adding debian:SZAFIR_ROOT_CA2.pem\n",
      "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
      "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
      "Adding debian:SecureSign_RootCA11.pem\n",
      "Adding debian:SecureTrust_CA.pem\n",
      "Adding debian:Secure_Global_CA.pem\n",
      "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
      "Adding debian:Security_Communication_RootCA2.pem\n",
      "Adding debian:Security_Communication_RootCA3.pem\n",
      "Adding debian:Security_Communication_Root_CA.pem\n",
      "Adding debian:Starfield_Class_2_CA.pem\n",
      "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
      "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
      "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
      "Adding debian:TWCA_Global_Root_CA.pem\n",
      "Adding debian:TWCA_Root_Certification_Authority.pem\n",
      "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
      "Adding debian:Telia_Root_CA_v2.pem\n",
      "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
      "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
      "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
      "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
      "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
      "Adding debian:TunTrust_Root_CA.pem\n",
      "Adding debian:UCA_Extended_Validation_Root.pem\n",
      "Adding debian:UCA_Global_G2_Root.pem\n",
      "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
      "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
      "Adding debian:XRamp_Global_CA_Root.pem\n",
      "Adding debian:certSIGN_ROOT_CA.pem\n",
      "Adding debian:certSIGN_Root_CA_G2.pem\n",
      "Adding debian:e-Szigno_Root_CA_2017.pem\n",
      "Adding debian:ePKI_Root_Certification_Authority.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
      "Adding debian:emSign_Root_CA_-_C1.pem\n",
      "Adding debian:emSign_Root_CA_-_G1.pem\n",
      "Adding debian:vTrus_ECC_Root_CA.pem\n",
      "Adding debian:vTrus_Root_CA.pem\n",
      "done.\n",
      "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~24.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
      "update-alternatives: warning: forcing reinstallation of alternative /usr/lib/jvm/java-11-openjdk-amd64/bin/java because link group java is broken\n",
      "update-alternatives: error: error creating symbolic link '/etc/alternatives/java.dpkg-tmp': Permission denied\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install konlpy\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y openjdk-11-jdk-headless\n",
    "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "import subprocess\n",
    "\n",
    "# Java ì„¤ì¹˜ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/default-java'\n",
    "os.environ['PATH'] = '/usr/lib/jvm/default-java/bin:' + os.environ.get('PATH', '')\n",
    "\n",
    "# ëŒ€ì•ˆ: Java ê²½ë¡œ ìë™ ì°¾ê¸°\n",
    "result = subprocess.run(['which', 'java'], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    java_path = result.stdout.strip()\n",
    "    java_home = java_path.replace('/bin/java', '')\n",
    "    os.environ['JAVA_HOME'] = java_home\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b26e5-0ce5-4d63-81de-1f979381bf91",
   "metadata": {},
   "source": [
    "ë°ì´í„° ë¡œë“œ ë° ë³‘í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d561a2e-c44d-4de4-acd2-13e172ae0fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>í˜‘ë°• ëŒ€í™”</td>\n",
       "      <td>ì§€ê¸ˆ ë„ˆ ìŠ¤ìŠ¤ë¡œë¥¼ ì£½ì—¬ë‹¬ë¼ê³  ì• ì›í•˜ëŠ” ê²ƒì¸ê°€?\\n ì•„ë‹™ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤.\\n ì£½ì„ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>í˜‘ë°• ëŒ€í™”</td>\n",
       "      <td>ê¸¸ë™ê²½ì°°ì„œì…ë‹ˆë‹¤.\\n9ì‹œ 40ë¶„ ë§ˆíŠ¸ì— í­ë°œë¬¼ì„ ì„¤ì¹˜í• ê±°ë‹¤.\\në„¤?\\në˜‘ë°”ë¡œ ë“¤ì–´ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ë„ˆ ë˜ê²Œ ê·€ì—¬ìš´ê±° ì•Œì§€? ë‚˜ë³´ë‹¤ ì‘ì€ ë‚¨ìëŠ” ì²¨ë´¤ì–´.\\nê·¸ë§Œí•´. ë‹ˆë“¤ ë†€ë¦¬ëŠ”ê±° ì¬ë¯¸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì–´ì´ ê±°ê¸°\\nì˜ˆ??\\në„ˆ ë§ì´ì•¼ ë„ˆ. ì´ë¦¬ ì˜¤ë¼ê³ \\në¬´ìŠ¨ ì¼.\\në„ˆ ì˜· ì¢‹ì•„ë³´ì¸ë‹¤?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì €ê¸°ìš” í˜¹ì‹œ ë‚ ì´ ë„ˆë¬´ ëœ¨ê²ì–ì•„ìš”? ì €í¬ íšŒì‚¬ì—ì„œ ì´ ì„ í¬ë¦¼ íŒŒëŠ”ë° í•œ ë²ˆ ì†ë“±ì— ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      class                                       conversation\n",
       "0           0      í˜‘ë°• ëŒ€í™”  ì§€ê¸ˆ ë„ˆ ìŠ¤ìŠ¤ë¡œë¥¼ ì£½ì—¬ë‹¬ë¼ê³  ì• ì›í•˜ëŠ” ê²ƒì¸ê°€?\\n ì•„ë‹™ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤.\\n ì£½ì„ ...\n",
       "1           1      í˜‘ë°• ëŒ€í™”  ê¸¸ë™ê²½ì°°ì„œì…ë‹ˆë‹¤.\\n9ì‹œ 40ë¶„ ë§ˆíŠ¸ì— í­ë°œë¬¼ì„ ì„¤ì¹˜í• ê±°ë‹¤.\\në„¤?\\në˜‘ë°”ë¡œ ë“¤ì–´ ...\n",
       "2           2  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ë„ˆ ë˜ê²Œ ê·€ì—¬ìš´ê±° ì•Œì§€? ë‚˜ë³´ë‹¤ ì‘ì€ ë‚¨ìëŠ” ì²¨ë´¤ì–´.\\nê·¸ë§Œí•´. ë‹ˆë“¤ ë†€ë¦¬ëŠ”ê±° ì¬ë¯¸...\n",
       "3           3      ê°ˆì·¨ ëŒ€í™”  ì–´ì´ ê±°ê¸°\\nì˜ˆ??\\në„ˆ ë§ì´ì•¼ ë„ˆ. ì´ë¦¬ ì˜¤ë¼ê³ \\në¬´ìŠ¨ ì¼.\\në„ˆ ì˜· ì¢‹ì•„ë³´ì¸ë‹¤?...\n",
       "4           4      ê°ˆì·¨ ëŒ€í™”  ì €ê¸°ìš” í˜¹ì‹œ ë‚ ì´ ë„ˆë¬´ ëœ¨ê²ì–ì•„ìš”? ì €í¬ íšŒì‚¬ì—ì„œ ì´ ì„ í¬ë¦¼ íŒŒëŠ”ë° í•œ ë²ˆ ì†ë“±ì— ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('./data/train_w_general_conv.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ca35db-971e-4e83-bf0e-31e4cc036862",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed7f55b-0861-4abc-8ff7-5b73d9dd9f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>í˜‘ë°• ëŒ€í™”</td>\n",
       "      <td>ì§€ê¸ˆ ë„ˆ ìŠ¤ìŠ¤ë¡œë¥¼ ì£½ì—¬ë‹¬ë¼ê³  ì• ì›í•˜ëŠ” ê²ƒì¸ê°€?\\n ì•„ë‹™ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤.\\n ì£½ì„ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>í˜‘ë°• ëŒ€í™”</td>\n",
       "      <td>ê¸¸ë™ê²½ì°°ì„œì…ë‹ˆë‹¤.\\n9ì‹œ 40ë¶„ ë§ˆíŠ¸ì— í­ë°œë¬¼ì„ ì„¤ì¹˜í• ê±°ë‹¤.\\në„¤?\\në˜‘ë°”ë¡œ ë“¤ì–´ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ë„ˆ ë˜ê²Œ ê·€ì—¬ìš´ê±° ì•Œì§€? ë‚˜ë³´ë‹¤ ì‘ì€ ë‚¨ìëŠ” ì²¨ë´¤ì–´.\\nê·¸ë§Œí•´. ë‹ˆë“¤ ë†€ë¦¬ëŠ”ê±° ì¬ë¯¸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì–´ì´ ê±°ê¸°\\nì˜ˆ??\\në„ˆ ë§ì´ì•¼ ë„ˆ. ì´ë¦¬ ì˜¤ë¼ê³ \\në¬´ìŠ¨ ì¼.\\në„ˆ ì˜· ì¢‹ì•„ë³´ì¸ë‹¤?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì €ê¸°ìš” í˜¹ì‹œ ë‚ ì´ ë„ˆë¬´ ëœ¨ê²ì–ì•„ìš”? ì €í¬ íšŒì‚¬ì—ì„œ ì´ ì„ í¬ë¦¼ íŒŒëŠ”ë° í•œ ë²ˆ ì†ë“±ì— ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                       conversation\n",
       "0      í˜‘ë°• ëŒ€í™”  ì§€ê¸ˆ ë„ˆ ìŠ¤ìŠ¤ë¡œë¥¼ ì£½ì—¬ë‹¬ë¼ê³  ì• ì›í•˜ëŠ” ê²ƒì¸ê°€?\\n ì•„ë‹™ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤.\\n ì£½ì„ ...\n",
       "1      í˜‘ë°• ëŒ€í™”  ê¸¸ë™ê²½ì°°ì„œì…ë‹ˆë‹¤.\\n9ì‹œ 40ë¶„ ë§ˆíŠ¸ì— í­ë°œë¬¼ì„ ì„¤ì¹˜í• ê±°ë‹¤.\\në„¤?\\në˜‘ë°”ë¡œ ë“¤ì–´ ...\n",
       "2  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ë„ˆ ë˜ê²Œ ê·€ì—¬ìš´ê±° ì•Œì§€? ë‚˜ë³´ë‹¤ ì‘ì€ ë‚¨ìëŠ” ì²¨ë´¤ì–´.\\nê·¸ë§Œí•´. ë‹ˆë“¤ ë†€ë¦¬ëŠ”ê±° ì¬ë¯¸...\n",
       "3      ê°ˆì·¨ ëŒ€í™”  ì–´ì´ ê±°ê¸°\\nì˜ˆ??\\në„ˆ ë§ì´ì•¼ ë„ˆ. ì´ë¦¬ ì˜¤ë¼ê³ \\në¬´ìŠ¨ ì¼.\\në„ˆ ì˜· ì¢‹ì•„ë³´ì¸ë‹¤?...\n",
       "4      ê°ˆì·¨ ëŒ€í™”  ì €ê¸°ìš” í˜¹ì‹œ ë‚ ì´ ë„ˆë¬´ ëœ¨ê²ì–ì•„ìš”? ì €í¬ íšŒì‚¬ì—ì„œ ì´ ì„ í¬ë¦¼ íŒŒëŠ”ë° í•œ ë²ˆ ì†ë“±ì— ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe97c69-9306-4780-a338-93dc3a1526f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ê°ˆì·¨ ëŒ€í™”</th>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</th>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì¼ë°˜ ëŒ€í™”</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”</th>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>í˜‘ë°• ëŒ€í™”</th>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             conversation\n",
       "class                    \n",
       "ê°ˆì·¨ ëŒ€í™”                 981\n",
       "ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”            1094\n",
       "ì¼ë°˜ ëŒ€í™”                1000\n",
       "ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”           979\n",
       "í˜‘ë°• ëŒ€í™”                 896"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8153fc19-4830-460c-8560-f021f9696154",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "stop_words = {\"í•˜ë‹¤\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c9e094-e047-42b4-86d1-fbbf62b7d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, stop_words):\n",
    "    # 1. ì–‘ìª½ ê³µë°± ì œê±°\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 2. íŠ¹ìˆ˜ë¬¸ì ë° ì´ëª¨ì§€ ì œê±° (í•œê¸€, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì ë§Œ í—ˆìš©)\n",
    "    sentence = re.sub(r\"[^ê°€-í£0-9a-zA-Z.,!?~\\s]\", \" \", sentence)\n",
    "\n",
    "    # 3. ì—°ì†ëœ ê³µë°± í•˜ë‚˜ë¡œ ì¶•ì†Œ ë° ì¤„ ë°”ê¿ˆ ë¬´ì‹œ\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\n\", \" \", sentence)\n",
    "\n",
    "    # 4. ë¬¸ì¥ ë¶€í˜¸ ì•ë’¤ë¡œ ê³µë°± ì¶”ê°€ (í† í° êµ¬ë¶„ì„ ìœ„í•¨)\n",
    "    sentence = re.sub(r\"([?.!,~])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "    \n",
    "    # í˜•íƒœì†Œ ë¶„ì„ (ë‹¨ì–´, í’ˆì‚¬)\n",
    "    include_tags = {\"Noun\", \"Verb\", \"Adjective\", \"Exclamation\", \"Adverb\"}\n",
    "    pos_tags = okt.pos(sentence, stem=True, norm=True)\n",
    "    # ì›í•˜ëŠ” í’ˆì‚¬ë§Œ ì¶”ì¶œ\n",
    "    tokens = [\n",
    "        word for word, tag in pos_tags\n",
    "        if tag in include_tags and len(word) > 1 and word not in stop_words\n",
    "    ]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fc93fd-9f7f-451f-b5c3-a8c4930401c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì§€ê¸ˆ', 'ìŠ¤ìŠ¤ë¡œ', 'ì£½ì´ë‹¤', 'ë‹¬ë¼', 'ì• ì›', 'ì•„ë‹ˆë‹¤', 'ì£„ì†¡í•˜ë‹¤', 'í˜¼ì', 'ì£½ì§€', 'ìš°ë¦¬', 'ì‚¬ê±´', 'ë§ë¦¬', 'ì§„ì§œ', 'ì£½ì´ë‹¤', 'ë²„ë¦¬ë‹¤', 'ì‹¶ë‹¤', 'ì •ë§', 'ì„ íƒ', 'ì£½ë‹¤', 'ê°€ì¡±', 'ì£½ì—¬ì£¼ë‹¤', 'ì£„ì†¡í•˜ë‹¤', 'ì •ë§', 'ì„ íƒ', 'ì—†ë‹¤', 'ì„ íƒ', 'ê°€ì¡±', 'ëª¨ì¡°ë¦¬', 'ì£½ì´ë‹¤', 'ë²„ë¦¬ë‹¤', 'ì„ íƒ', 'í•œë²ˆ', 'ë„ì™€ì£¼ë‹¤', 'ê·¸ëƒ¥', 'ì£½ì´ë‹¤', 'ë²„ë¦¬ë‹¤', 'ì´ì˜', 'ì—†ë‹¤', 'ì œë°œ', 'ë„ì™€ì£¼ë‹¤']\n"
     ]
    }
   ],
   "source": [
    "sample_text = raw_data['conversation'][0]\n",
    "tokens = preprocess_sentence(sample_text, stop_words)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9bb624-1ee9-4f25-a097-6ee893d00f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>í˜‘ë°• ëŒ€í™”</td>\n",
       "      <td>ì§€ê¸ˆ ë„ˆ ìŠ¤ìŠ¤ë¡œë¥¼ ì£½ì—¬ë‹¬ë¼ê³  ì• ì›í•˜ëŠ” ê²ƒì¸ê°€?\\n ì•„ë‹™ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤.\\n ì£½ì„ ...</td>\n",
       "      <td>[ì§€ê¸ˆ, ìŠ¤ìŠ¤ë¡œ, ì£½ì´ë‹¤, ë‹¬ë¼, ì• ì›, ì•„ë‹ˆë‹¤, ì£„ì†¡í•˜ë‹¤, í˜¼ì, ì£½ì§€, ìš°ë¦¬, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>í˜‘ë°• ëŒ€í™”</td>\n",
       "      <td>ê¸¸ë™ê²½ì°°ì„œì…ë‹ˆë‹¤.\\n9ì‹œ 40ë¶„ ë§ˆíŠ¸ì— í­ë°œë¬¼ì„ ì„¤ì¹˜í• ê±°ë‹¤.\\në„¤?\\në˜‘ë°”ë¡œ ë“¤ì–´ ...</td>\n",
       "      <td>[ê¸¸ë™, ê²½ì°°ì„œ, ì´ë‹¤, ë§ˆíŠ¸, í­ë°œë¬¼, ì„¤ì¹˜, ë˜‘ë°”ë¡œ, ë“¤ë‹¤, í•œë²ˆ, ì–˜ê¸°, ì¥ë‚œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ë„ˆ ë˜ê²Œ ê·€ì—¬ìš´ê±° ì•Œì§€? ë‚˜ë³´ë‹¤ ì‘ì€ ë‚¨ìëŠ” ì²¨ë´¤ì–´.\\nê·¸ë§Œí•´. ë‹ˆë“¤ ë†€ë¦¬ëŠ”ê±° ì¬ë¯¸...</td>\n",
       "      <td>[ë˜ê²Œ, ê·€ì—½ë‹¤, ì‘ë‹¤, ë‚¨ì, ë³´ë‹¤, ê·¸ë§Œí•˜ë‹¤, ë†€ë¦¬ë‹¤, ì¬ë¯¸ì—†ë‹¤, ì§€ì˜, ì´ì§€,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì–´ì´ ê±°ê¸°\\nì˜ˆ??\\në„ˆ ë§ì´ì•¼ ë„ˆ. ì´ë¦¬ ì˜¤ë¼ê³ \\në¬´ìŠ¨ ì¼.\\në„ˆ ì˜· ì¢‹ì•„ë³´ì¸ë‹¤?...</td>\n",
       "      <td>[ì–´ì´, ê±°ê¸°, ì´ë¦¬, ì˜¤ë¼, ë¬´ìŠ¨, ì¢‹ë‹¤, ë³´ì´ë‹¤, ìˆë‹¤, ë³´ë‹¤, ì•„ë‹ˆë‹¤, ì—†ë‹¤,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì €ê¸°ìš” í˜¹ì‹œ ë‚ ì´ ë„ˆë¬´ ëœ¨ê²ì–ì•„ìš”? ì €í¬ íšŒì‚¬ì—ì„œ ì´ ì„ í¬ë¦¼ íŒŒëŠ”ë° í•œ ë²ˆ ì†ë“±ì— ...</td>\n",
       "      <td>[ì €ê¸°, í˜¹ì‹œ, ë„ˆë¬´, ëœ¨ê²ë‹¤, ì €í¬, íšŒì‚¬, ì„ í¬ë¦¼, íŒ”ë‹¤, ì†ë“±, ë°œë¼, ë³´ë‹¤,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                       conversation  \\\n",
       "0      í˜‘ë°• ëŒ€í™”  ì§€ê¸ˆ ë„ˆ ìŠ¤ìŠ¤ë¡œë¥¼ ì£½ì—¬ë‹¬ë¼ê³  ì• ì›í•˜ëŠ” ê²ƒì¸ê°€?\\n ì•„ë‹™ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤.\\n ì£½ì„ ...   \n",
       "1      í˜‘ë°• ëŒ€í™”  ê¸¸ë™ê²½ì°°ì„œì…ë‹ˆë‹¤.\\n9ì‹œ 40ë¶„ ë§ˆíŠ¸ì— í­ë°œë¬¼ì„ ì„¤ì¹˜í• ê±°ë‹¤.\\në„¤?\\në˜‘ë°”ë¡œ ë“¤ì–´ ...   \n",
       "2  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ë„ˆ ë˜ê²Œ ê·€ì—¬ìš´ê±° ì•Œì§€? ë‚˜ë³´ë‹¤ ì‘ì€ ë‚¨ìëŠ” ì²¨ë´¤ì–´.\\nê·¸ë§Œí•´. ë‹ˆë“¤ ë†€ë¦¬ëŠ”ê±° ì¬ë¯¸...   \n",
       "3      ê°ˆì·¨ ëŒ€í™”  ì–´ì´ ê±°ê¸°\\nì˜ˆ??\\në„ˆ ë§ì´ì•¼ ë„ˆ. ì´ë¦¬ ì˜¤ë¼ê³ \\në¬´ìŠ¨ ì¼.\\në„ˆ ì˜· ì¢‹ì•„ë³´ì¸ë‹¤?...   \n",
       "4      ê°ˆì·¨ ëŒ€í™”  ì €ê¸°ìš” í˜¹ì‹œ ë‚ ì´ ë„ˆë¬´ ëœ¨ê²ì–ì•„ìš”? ì €í¬ íšŒì‚¬ì—ì„œ ì´ ì„ í¬ë¦¼ íŒŒëŠ”ë° í•œ ë²ˆ ì†ë“±ì— ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [ì§€ê¸ˆ, ìŠ¤ìŠ¤ë¡œ, ì£½ì´ë‹¤, ë‹¬ë¼, ì• ì›, ì•„ë‹ˆë‹¤, ì£„ì†¡í•˜ë‹¤, í˜¼ì, ì£½ì§€, ìš°ë¦¬, ...  \n",
       "1  [ê¸¸ë™, ê²½ì°°ì„œ, ì´ë‹¤, ë§ˆíŠ¸, í­ë°œë¬¼, ì„¤ì¹˜, ë˜‘ë°”ë¡œ, ë“¤ë‹¤, í•œë²ˆ, ì–˜ê¸°, ì¥ë‚œ...  \n",
       "2  [ë˜ê²Œ, ê·€ì—½ë‹¤, ì‘ë‹¤, ë‚¨ì, ë³´ë‹¤, ê·¸ë§Œí•˜ë‹¤, ë†€ë¦¬ë‹¤, ì¬ë¯¸ì—†ë‹¤, ì§€ì˜, ì´ì§€,...  \n",
       "3  [ì–´ì´, ê±°ê¸°, ì´ë¦¬, ì˜¤ë¼, ë¬´ìŠ¨, ì¢‹ë‹¤, ë³´ì´ë‹¤, ìˆë‹¤, ë³´ë‹¤, ì•„ë‹ˆë‹¤, ì—†ë‹¤,...  \n",
       "4  [ì €ê¸°, í˜¹ì‹œ, ë„ˆë¬´, ëœ¨ê²ë‹¤, ì €í¬, íšŒì‚¬, ì„ í¬ë¦¼, íŒ”ë‹¤, ì†ë“±, ë°œë¼, ë³´ë‹¤,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['tokens'] = raw_data['conversation'].apply(lambda x: preprocess_sentence(str(x), stop_words))\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "595a09bf-724d-41b8-8bce-fa7d90b907c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ì•ˆë…•í•˜ì„¸ìš”!! ì €ëŠ” ë°ì´í„° ë¶„ì„ì„ ì¢‹ì•„í•´ìš”~~ ğŸ˜Š\n",
      "ì „ì²˜ë¦¬ ê²°ê³¼: ['ì•ˆë…•í•˜ë‹¤', 'ë°ì´í„°', 'ë¶„ì„', 'ì¢‹ì•„í•˜ë‹¤']\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"ì•ˆë…•í•˜ì„¸ìš”!! ì €ëŠ” ë°ì´í„° ë¶„ì„ì„ ì¢‹ì•„í•´ìš”~~ ğŸ˜Š\"\n",
    "result = preprocess_sentence(sample_sentence, stop_words)\n",
    "print(f\"ì›ë¬¸: {sample_sentence}\")\n",
    "print(f\"ì „ì²˜ë¦¬ ê²°ê³¼: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cd9eb3-e5cc-4033-b9ce-1bbcfd238aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) Vocab ë¹Œë“œ =====\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "import json\n",
    "\n",
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<cls>\", \"<sep>\"]\n",
    "\n",
    "def build_vocab(\n",
    "    token_lists: Iterable[List[str]],\n",
    "    min_freq: int = 2,\n",
    "    max_size: int = 30000,\n",
    "    specials: List[str] = SPECIALS,\n",
    ") -> Tuple[Dict[str, int], List[str], Counter]:\n",
    "    \"\"\"\n",
    "    token_lists: ê° ìƒ˜í”Œì˜ í† í° ë¦¬ìŠ¤íŠ¸(iterable of list[str])\n",
    "    min_freq: ìµœì†Œ ë“±ì¥ ë¹ˆë„ ë¯¸ë§Œ í† í°ì€ ì œì™¸\n",
    "    max_size: special í¬í•¨ ì „ì²´ vocab ìƒí•œ (Noneì´ë©´ ì œí•œ ì—†ìŒ)\n",
    "    returns: (stoi, itos, counter)\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for toks in token_lists:\n",
    "        counter.update(toks)\n",
    "\n",
    "    # ë¹ˆë„ í•„í„° + ìƒìœ„ max_size-íŠ¹ìˆ˜í† í° ë§Œí¼\n",
    "    most = [tok for tok, cnt in counter.most_common() if cnt >= min_freq]\n",
    "    if max_size is not None:\n",
    "        cap = max_size - len(specials)\n",
    "        most = most[:max(0, cap)]\n",
    "\n",
    "    itos = list(specials) + most\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos, counter\n",
    "\n",
    "def save_vocab(path: str, itos: List[str]) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(itos, f, ensure_ascii=False)\n",
    "\n",
    "def load_vocab(path: str) -> Tuple[Dict[str, int], List[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        itos = json.load(f)\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7637ca8-9d04-48e7-9b37-1a3a15a8b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2) í† í° â†’ ID ì¸ì½”ë”© =====\n",
    "def encode_tokens(\n",
    "    tokens: List[str],\n",
    "    stoi: Dict[str, int],\n",
    "    max_len: int = 256,\n",
    "    add_cls: bool = True,\n",
    "    add_sep: bool = True,\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    tokens -> input_ids, attention_mask\n",
    "    - OOVëŠ” <unk>\n",
    "    - <cls>, <sep>ë¥¼ ì˜µì…˜ìœ¼ë¡œ ì•/ë’¤ì— ë¶€ì°©\n",
    "    - max_lenì„ ì´ˆê³¼í•˜ë©´ ì ì ˆíˆ ìë¦„\n",
    "    \"\"\"\n",
    "    pad_id = stoi[\"<pad>\"]\n",
    "    unk_id = stoi[\"<unk>\"]\n",
    "    cls_id = stoi.get(\"<cls>\")\n",
    "    sep_id = stoi.get(\"<sep>\")\n",
    "\n",
    "    ids = [stoi.get(t, unk_id) for t in tokens]\n",
    "\n",
    "    # ê¸¸ì´ ê³„ì‚° (cls/sep í¬í•¨í•´ì„œ ìë¥´ê¸°)\n",
    "    extra = (1 if add_cls else 0) + (1 if add_sep else 0)\n",
    "    keep = max_len - extra\n",
    "    keep = max(0, keep)\n",
    "    ids = ids[:keep]\n",
    "\n",
    "    if add_cls:\n",
    "        ids = [cls_id] + ids\n",
    "    if add_sep:\n",
    "        ids = ids + [sep_id]\n",
    "\n",
    "    attn = [1] * len(ids)\n",
    "    return ids, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59b1c200-4f8e-4fa8-aa82-bf82675872f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3) ë°°ì¹˜ íŒ¨ë”©(collate) =====\n",
    "import torch\n",
    "\n",
    "def collate_batch(\n",
    "    batch,\n",
    "    pad_id: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    batch: [{\"input_ids\": List[int], \"attention_mask\": List[int], \"label\": int}, ...]\n",
    "    \"\"\"\n",
    "    bs = len(batch)\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids = torch.full((bs, maxlen), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((bs, maxlen), dtype=torch.long)\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "\n",
    "    for i, x in enumerate(batch):\n",
    "        L = len(x[\"input_ids\"])\n",
    "        input_ids[i, :L] = torch.tensor(x[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask[i, :L] = torch.tensor(x[\"attention_mask\"], dtype=torch.long)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d543b95c-15b7-4ee6-bce4-4d817f85e78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9439 | Labels: {'í˜‘ë°• ëŒ€í™”': 0, 'ê°ˆì·¨ ëŒ€í™”': 1, 'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”': 2, 'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”': 3, 'ì¼ë°˜ ëŒ€í™”': 4}\n",
      "torch.Size([16, 60])\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) ì˜ˆì‹œ íŒŒì´í”„ë¼ì¸ (ë¼ë²¨ ë§¤í•‘ í¬í•¨) =====\n",
    "# 4-1) tokens ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ë¨¼ì € ìƒì„±\n",
    "# raw_data['tokens'] = raw_data['conversation'].apply(lambda s: preprocess_sentence(str(s), stop_words))\n",
    "\n",
    "# 4-2) ë¼ë²¨ ë§¤í•‘\n",
    "labels = sorted(raw_data[\"class\"].unique().tolist())\n",
    "label2id = {\n",
    "    \"í˜‘ë°• ëŒ€í™”\": 0,\n",
    "    \"ê°ˆì·¨ ëŒ€í™”\": 1,\n",
    "    \"ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”\": 2,\n",
    "    \"ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”\": 3,\n",
    "    \"ì¼ë°˜ ëŒ€í™”\": 4,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# 4-3) vocab ë¹Œë“œ\n",
    "stoi, itos, counter = build_vocab(raw_data[\"tokens\"], min_freq=1, max_size=20000)\n",
    "pad_id = stoi[\"<pad>\"]\n",
    "\n",
    "# 4-4) ì¸ì½”ë”© (train/valid ë¶„í• ì€ ì´ë¯¸ ë˜ì–´ìˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜ ì•„ë˜ì²˜ëŸ¼ ê°„ë‹¨ ë¶„í• )\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(raw_data, test_size=0.2, random_state=42, stratify=raw_data[\"class\"])\n",
    "\n",
    "def encode_row(row, max_len=256):\n",
    "    ids, attn = encode_tokens(row[\"tokens\"], stoi, max_len=max_len, add_cls=True, add_sep=True)\n",
    "    return {\n",
    "        \"input_ids\": ids,\n",
    "        \"attention_mask\": attn,\n",
    "        \"label\": label2id[row[\"class\"]],\n",
    "    }\n",
    "\n",
    "train_records = [encode_row(r) for _, r in train_df.iterrows()]\n",
    "valid_records = [encode_row(r) for _, r in valid_df.iterrows()]\n",
    "\n",
    "# 4-5) PyTorch Dataset/Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleListDataset(Dataset):\n",
    "    def __init__(self, records):\n",
    "        self.records = records\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.records[idx]\n",
    "\n",
    "train_ds = SimpleListDataset(train_records)\n",
    "valid_ds = SimpleListDataset(valid_records)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
    "                          collate_fn=lambda b: collate_batch(b, pad_id))\n",
    "valid_loader = DataLoader(valid_ds, batch_size=32, shuffle=False,\n",
    "                          collate_fn=lambda b: collate_batch(b, pad_id))\n",
    "\n",
    "print(f\"Vocab size: {len(itos)} | Labels: {label2id}\")\n",
    "print(next(iter(train_loader))[\"input_ids\"].shape)  # (B, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "677cdf4b-b909-407a-9c0d-0ee89d71d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LSTM Classifier ì •ì˜ =====\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM ê¸°ë°˜ ë¬¸ì„œ/ëŒ€í™” ë¶„ë¥˜ê¸°\n",
    "    - input_ids: (B, S) í† í° ì¸ë±ìŠ¤\n",
    "    - attention_mask: (B, S) 1=ìœ íš¨, 0=íŒ¨ë”©\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_classes: int,\n",
    "        emb_dim: int = 256,\n",
    "        hidden_dim: int = 512,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        pad_id: int = 0,\n",
    "        bidirectional: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.norm = nn.LayerNorm(lstm_output_dim)\n",
    "        self.classifier = nn.Linear(lstm_output_dim, num_classes)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        return_repr: bool = False,\n",
    "    ):\n",
    "        x = self.emb(input_ids)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1).cpu()\n",
    "            x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            lstm_out, (h_n, c_n) = self.lstm(x_packed)\n",
    "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        else:\n",
    "            lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        if self.lstm.bidirectional:\n",
    "            sent_repr = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        else:\n",
    "            sent_repr = h_n[-1]\n",
    "        \n",
    "        sent_repr = self.norm(sent_repr)\n",
    "        logits = self.classifier(sent_repr)\n",
    "        \n",
    "        if return_repr:\n",
    "            return logits, sent_repr\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1197f8aa-ba5f-46f8-8b6f-04317b9fab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== í•™ìŠµ/í‰ê°€ ë£¨í”„ =====\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    class_weights: torch.Tensor = None,\n",
    "    grad_clip: float = 1.0,\n",
    "    scheduler = None,\n",
    "    use_amp: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    ce = nn.CrossEntropyLoss(weight=class_weights.to(device) if class_weights is not None else None)\n",
    "    \n",
    "    losses, all_preds, all_labels = [], [], []\n",
    "    for batch in tqdm(dataloader, desc=\"train\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch.get(\"attention_mask\")\n",
    "        attn = attn.to(device) if attn is not None else None\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(input_ids, attention_mask=attn)\n",
    "            loss = ce(logits, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        if grad_clip is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        all_preds += logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "        all_labels += labels.detach().cpu().tolist()\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return {\"loss\": sum(losses)/len(losses), \"acc\": acc, \"f1_macro\": f1}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses, all_preds, all_labels = [], [], []\n",
    "    for batch in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch.get(\"attention_mask\")\n",
    "        attn = attn.to(device) if attn is not None else None\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask=attn)\n",
    "        loss = ce(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        all_preds += logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "        all_labels += labels.detach().cpu().tolist()\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\") if all_labels else 0.0\n",
    "    return {\"loss\": sum(losses)/len(losses), \"acc\": acc, \"f1_macro\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac0e3025-1199-4ff4-b6ac-7a94d0344e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ëª¨ë¸ íŒ©í† ë¦¬ =====\n",
    "def create_model(\n",
    "    vocab_size: int,\n",
    "    num_classes: int = 5,\n",
    "    pad_id: int = 0,\n",
    "    emb_dim: int = 256,\n",
    "    hidden_dim: int = 512,\n",
    "    num_layers: int = 2,\n",
    "    dropout: float = 0.1,\n",
    "    bidirectional: bool = True,\n",
    ") -> nn.Module:\n",
    "    return LSTMClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        emb_dim=emb_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        pad_id=pad_id,\n",
    "        bidirectional=bidirectional,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43296a53-2aba-4759-97ff-23eec8b174c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ëª¨ë¸ ìƒì„± ë° ì„¤ì • =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = create_model(\n",
    "    vocab_size=len(itos),\n",
    "    num_classes=5,\n",
    "    pad_id=stoi[\"<pad>\"],\n",
    "    emb_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    bidirectional=True,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
    "scheduler = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f9b324b-7616-4570-a7da-61890458a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231bdd4fc1474da19a7544c0a7803aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ead1dca1d8c483c9fb35c66d4a90ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train: {'loss': 0.9966348606732583, 'acc': 0.6323232323232323, 'f1_macro': 0.6319555399990169} | valid: {'loss': 0.6773548981835765, 'acc': 0.7232323232323232, 'f1_macro': 0.7179981945414989}\n",
      "  âœ” saved best model (F1 â†‘)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0e9f6400764af59f34cb825ef6c846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ca3154364e4c71ab2fc829e52744ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02] train: {'loss': 0.5179249368487827, 'acc': 0.8126262626262626, 'f1_macro': 0.8124893034691162} | valid: {'loss': 0.6451728569884454, 'acc': 0.7727272727272727, 'f1_macro': 0.7672332203276919}\n",
      "  âœ” saved best model (F1 â†‘)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b6fb4b8fcd4872a8c28f9502aef14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bc6ad96a654c56b6477c0b221823b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03] train: {'loss': 0.3461589119134231, 'acc': 0.876010101010101, 'f1_macro': 0.8757524083978273} | valid: {'loss': 0.7434164309693921, 'acc': 0.7848484848484848, 'f1_macro': 0.7812240538347706}\n",
      "  âœ” saved best model (F1 â†‘)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39322906e10e42b3bd3f973627f70419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b23926df0cb4fb6b83cfbf87f21ad47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04] train: {'loss': 0.21939256086179446, 'acc': 0.9285353535353535, 'f1_macro': 0.928615906186908} | valid: {'loss': 0.7347903905376312, 'acc': 0.8191919191919191, 'f1_macro': 0.8181514477102491}\n",
      "  âœ” saved best model (F1 â†‘)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4577cc6f022e46609a0f8969c432e0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72282ceaeded4ca78d3deb687896dc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05] train: {'loss': 0.1441357102331465, 'acc': 0.9497474747474748, 'f1_macro': 0.9495677346014274} | valid: {'loss': 1.084198559484174, 'acc': 0.7787878787878788, 'f1_macro': 0.7805903195236762}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9024c214e8b543daa391beeafd23cfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5a909cc3bc4db098d467dc616b1bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06] train: {'loss': 0.0978086412371428, 'acc': 0.9664141414141414, 'f1_macro': 0.9667062652136487} | valid: {'loss': 1.2918495987692187, 'acc': 0.8141414141414142, 'f1_macro': 0.8145825421471733}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba75dbdebbe40d29b3382a5f61ba81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28607c42769b48a7b7ceca802f161fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07] train: {'loss': 0.07859221386950359, 'acc': 0.9767676767676767, 'f1_macro': 0.9767620492457745} | valid: {'loss': 1.3696297397536616, 'acc': 0.8121212121212121, 'f1_macro': 0.810972843955077}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ee3578665a4e79833b46a62f4e18bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacc5da77cfc4134b19fefe223e38db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08] train: {'loss': 0.06775326677467869, 'acc': 0.9818181818181818, 'f1_macro': 0.9815768656818513} | valid: {'loss': 1.5669387790464586, 'acc': 0.8181818181818182, 'f1_macro': 0.8197444982925927}\n",
      "  âœ” saved best model (F1 â†‘)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7e662fd0fe4b36afbacba7d41160c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcad505c0a2b49edb9c84dcb450d485f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09] train: {'loss': 0.04367041426560811, 'acc': 0.9873737373737373, 'f1_macro': 0.9874902646291608} | valid: {'loss': 1.432207149122992, 'acc': 0.8252525252525252, 'f1_macro': 0.8266509203138022}\n",
      "  âœ” saved best model (F1 â†‘)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34524e4337004c32aab7d4ae5a2d21cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8c6b2de0a04c18b68288cad87e8299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] train: {'loss': 0.052230302654554674, 'acc': 0.9866161616161616, 'f1_macro': 0.9865879658026575} | valid: {'loss': 1.4813883458414385, 'acc': 0.8333333333333334, 'f1_macro': 0.8346609304479564}\n",
      "  âœ” saved best model (F1 â†‘)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378fd06ef23e462f851f5d7abdbf7ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1463d00a23e4da69576d1271f49297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] train: {'loss': 0.041003736948453356, 'acc': 0.9891414141414141, 'f1_macro': 0.9891379430959143} | valid: {'loss': 1.7137352074346235, 'acc': 0.8292929292929293, 'f1_macro': 0.8283731069844851}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7885caf98d49d7b6a0b8e6d000b66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759fdd5010db4b09a7fbc4aca3a081e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12] train: {'loss': 0.019661354431870844, 'acc': 0.9934343434343434, 'f1_macro': 0.9933940933880484} | valid: {'loss': 1.7932680451100873, 'acc': 0.8242424242424242, 'f1_macro': 0.8241218792010724}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfc92cec95247fda989b7224b9fc97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff479dc67654a768986acab66b00aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] train: {'loss': 0.03270592023039821, 'acc': 0.9891414141414141, 'f1_macro': 0.9891424841108372} | valid: {'loss': 1.8740388731802664, 'acc': 0.8151515151515152, 'f1_macro': 0.8120228861657184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc74e5117b7e4560a09ea5d4b9a95be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d50f0d803647eb9c9b06d23fb56d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14] train: {'loss': 0.04715338876422887, 'acc': 0.9893939393939394, 'f1_macro': 0.989258224235796} | valid: {'loss': 1.854937372669097, 'acc': 0.8121212121212121, 'f1_macro': 0.810452873095304}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ad84cc04e94080a9d23fb7b5f952b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51d9cfbd2a94dddb47059020ed900f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] train: {'loss': 0.03820144178871366, 'acc': 0.9896464646464647, 'f1_macro': 0.9895825685019485} | valid: {'loss': 1.9144263757813362, 'acc': 0.804040404040404, 'f1_macro': 0.802999082353583}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78be55a32dd443fa35c9ad73170c706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6916a1404e25462491c63d0789c4a6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] train: {'loss': 0.03279968817777354, 'acc': 0.9911616161616161, 'f1_macro': 0.9910883277146094} | valid: {'loss': 2.039685099355636, 'acc': 0.8202020202020202, 'f1_macro': 0.8184508421140627}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d49a36ad564cf995bcff4f21aaf1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bd51d6c51c4717a87eb536613cc508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] train: {'loss': 0.02853334082142844, 'acc': 0.9931818181818182, 'f1_macro': 0.9932454379097653} | valid: {'loss': 2.0540981331179218, 'acc': 0.8171717171717172, 'f1_macro': 0.8152097843614712}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4211e445b24e679d91fc5f87bac5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63a5c972973498da4e8f28a18d8a8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] train: {'loss': 0.02149061174970458, 'acc': 0.9939393939393939, 'f1_macro': 0.993972416352638} | valid: {'loss': 1.968508142617441, 'acc': 0.8252525252525252, 'f1_macro': 0.8241573475410202}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f043549e5a40fabe64929fae3645bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b44e1338734f088636d831e8bf9e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] train: {'loss': 0.018747025452806664, 'acc': 0.9946969696969697, 'f1_macro': 0.9947199989482867} | valid: {'loss': 2.141779854413002, 'acc': 0.8343434343434344, 'f1_macro': 0.8327467239579184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476504c505814429b0d1b1b50929d824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213/3461024018.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e40a9fe248643b284422d47bf42a82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] train: {'loss': 0.022355927340190207, 'acc': 0.9926767676767677, 'f1_macro': 0.9925922256549864} | valid: {'loss': 2.152456272032953, 'acc': 0.8292929292929293, 'f1_macro': 0.8286561985953383}\n"
     ]
    }
   ],
   "source": [
    "# ===== í•™ìŠµ ì‹¤í–‰ =====\n",
    "from collections import Counter\n",
    "\n",
    "train_labels = [rec[\"label\"] for rec in train_records]\n",
    "cnt = Counter(train_labels)\n",
    "weights = torch.tensor([1.0 / max(cnt.get(i, 1), 1) for i in range(5)], dtype=torch.float)\n",
    "weights = weights / weights.mean()\n",
    "class_weights = weights\n",
    "\n",
    "best_f1 = 0.0\n",
    "epochs = 20\n",
    "for ep in range(1, epochs+1):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, device,\n",
    "                         class_weights=class_weights, grad_clip=1.0, scheduler=scheduler, use_amp=True)\n",
    "    va = evaluate(model, valid_loader, device)\n",
    "    print(f\"[{ep:02d}] train: {tr} | valid: {va}\")\n",
    "    \n",
    "    if va[\"f1_macro\"] > best_f1:\n",
    "        best_f1 = va[\"f1_macro\"]\n",
    "        torch.save(model.state_dict(), f\"./{best_f1:.4f}_best_lstm_cls.pt\")\n",
    "        print(\"  âœ” saved best model (F1 â†‘)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f0286-10f1-4be4-800b-866ff5aff595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5be008-8211-4595-a348-d1fec62dcc9b",
   "metadata": {},
   "source": [
    "# 0. Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cb8895ee-0b27-4b42-bb16-b968dc11e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import requests \n",
    "import zipfile \n",
    "import io \n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from typing import Dict, Any, Callable, Tuple, List, Optional\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a0173027-4811-4df4-977d-cb2f0837153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url: str = 'data/') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê²°í•©í•©ë‹ˆë‹¤. íŒŒì¼ì´ ì—†ìœ¼ë©´ GitHubì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"1. ë°ì´í„° ë¡œë“œ ì‹œì‘...\")\n",
    "    \n",
    "    # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    # íŒŒì¼ ì´ë¦„ì€ ìˆ˜ì •í•  ê²ƒ\n",
    "    required_files = ['train.csv', 'general_dialog1.csv', 'general_dialog2.csv', 'test.json']\n",
    "    all_files_exist = all(os.path.exists(os.path.join(url, f)) for f in required_files)\n",
    "\n",
    "    if not all_files_exist:\n",
    "        print(\"í•„ìˆ˜ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. GitHubì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "        download_url = \"https://github.com/tunib-ai/DKTC/archive/refs/heads/main.zip\"\n",
    "        \n",
    "        try:\n",
    "            # data ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "            os.makedirs(url, exist_ok=True)\n",
    "            \n",
    "            print(\"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "            response = requests.get(download_url)\n",
    "            response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ\n",
    "\n",
    "            print(\"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, ì••ì¶• í•´ì œ ì¤‘...\")\n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                for file in z.namelist():\n",
    "                    # DKTC-main/data/ í´ë” ì•ˆ íŒŒì¼ë§Œ ì¶”ì¶œ\n",
    "                    if file.startswith(\"DKTC-main/data/\") and not file.endswith('/'):\n",
    "                        # íŒŒì¼ ê²½ë¡œ ì¬ì„¤ì •: 'DKTC-main/data/íŒŒì¼ëª…' -> 'data/íŒŒì¼ëª…'\n",
    "                        target_path = os.path.join(url, os.path.relpath(file, \"DKTC-main/data\"))\n",
    "                        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "                        with z.open(file) as source, open(target_path, \"wb\") as target:\n",
    "                            target.write(source.read())\n",
    "            print(f\"'{url}' í´ë”ì— ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP ì˜¤ë¥˜ ë˜ëŠ” ì—°ê²° ë¬¸ì œ. {e}\")\n",
    "            raise FileNotFoundError(\"ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.\")\n",
    "        except Exception as e:\n",
    "            print(f\"íŒŒì¼ ì••ì¶• í•´ì œ/ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            raise FileNotFoundError(\"ë‹¤ìš´ë¡œë“œ í›„ íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "    # --- ë°ì´í„° ë¡œë“œ ì§„í–‰ (íŒŒì¼ì´ ì¡´ì¬í•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œ í›„) ---\n",
    "    train = pd.read_csv(os.path.join(url, 'train.csv'))\n",
    "    a = pd.read_csv(os.path.join(url, 'general_dialog1.csv')).rename(columns={'dialogue': 'conversation'})\n",
    "    a['class'] = 'ì¼ë°˜ ëŒ€í™”'\n",
    "    a['idx'] = range(0,len(a))\n",
    "    a = a[['idx','class','conversation']]\n",
    "    b = pd.read_csv(os.path.join(url, 'general_dialog2.csv'))\n",
    "    b = b[b['class'] == \"ì¼ë°˜ ëŒ€í™”\"] \n",
    "    train_df = pd.concat([train, a, b], axis=0, ignore_index=True).drop(columns='idx', errors='ignore')\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (test.json)\n",
    "    with open(os.path.join(url, \"test.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        test_df = pd.DataFrame({\"conversation\": [v[\"text\"] for v in json.load(f).values()]})\n",
    "    \n",
    "    print(f\"   - í›ˆë ¨ ë°ì´í„° shape: {train_df.shape}\")\n",
    "    print(f\"   - í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {test_df.shape}\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc48af-4e16-4945-ad3a-1d423f104ba8",
   "metadata": {},
   "source": [
    "# 1. Preprocessing & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26555d13-7dee-4cb9-bb93-fd23faadd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[int, str]]:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ìš”ì²­ëœ ê³ ì •ëœ ìˆ«ì ë ˆì´ë¸”ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í•¨ìˆ˜: íŠ¹ìˆ˜ ë¬¸ì ë° ë‹¤ì¤‘ ê³µë°± ì œê±° (ìƒëµ)\n",
    "    def clean_text(text: str) -> str:\n",
    "        text = str(text) \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE) \n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text \n",
    "\n",
    "    train_df['cleaned_conversation'] = train_df['conversation'].apply(clean_text)\n",
    "    test_df['cleaned_conversation'] = test_df['conversation'].apply(clean_text)\n",
    "\n",
    "    LABEL_MAPPING = {\n",
    "        'í˜‘ë°• ëŒ€í™”': 0,\n",
    "        'ê°ˆì·¨ ëŒ€í™”': 1,\n",
    "        'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”': 2,\n",
    "        'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”': 3,\n",
    "        'ì¼ë°˜ ëŒ€í™”': 4\n",
    "    }\n",
    "    \n",
    "    # ì¸ì½”ë”© ìˆ˜í–‰\n",
    "    train_df['label_encoded'] = train_df['class'].map(LABEL_MAPPING)\n",
    "    \n",
    "    print(f\"   - ë ˆì´ë¸” í´ë˜ìŠ¤: {len(LABEL_MAPPING)}ê°œ\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "07de5711-cf0a-46b1-8adc-f2ae6f5e74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "from typing import List, Callable\n",
    "\n",
    "# ì „ì—­ Okt ê°ì²´\n",
    "okt_global = Okt()\n",
    "\n",
    "# ì „ì—­ í•¨ìˆ˜ë¡œ ì •ì˜ (ë‚´ë¶€ í•¨ìˆ˜ X)\n",
    "def okt_tokenizer_morphs(text: str, stop_words: List[str] = None) -> List[str]:\n",
    "    if stop_words is None:\n",
    "        stop_words = []\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    tokens = okt_global.morphs(text, stem=True, norm=True)\n",
    "    return [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "\n",
    "def okt_tokenizer_nouns(text: str, stop_words: List[str] = None) -> List[str]:\n",
    "    if stop_words is None:\n",
    "        stop_words = []\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    tokens = okt_global.nouns(text)\n",
    "    return [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "\n",
    "def okt_tokenizer_pos(text: str, stop_words: List[str] = None) -> List[str]:\n",
    "    if stop_words is None:\n",
    "        stop_words = []\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    include_tags = {\"Noun\", \"Verb\", \"Adjective\", \"Exclamation\", \"Adverb\"}\n",
    "    pos_tags = okt_global.pos(text, stem=True, norm=True)\n",
    "    tokens = [\n",
    "        word for word, tag in pos_tags\n",
    "        if tag in include_tags and len(word) > 1 and word not in stop_words\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# ë§¤í•‘ ë”•ì…”ë„ˆë¦¬\n",
    "OKT_TOKENIZER_MAP = {\n",
    "    'morphs': okt_tokenizer_morphs,\n",
    "    'nouns': okt_tokenizer_nouns,\n",
    "    'pos': okt_tokenizer_pos\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4c727cfb-4945-404a-b77b-ccf0c29e92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_tfidf(\n",
    "    train_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame, \n",
    "    tokenizer_name: str, \n",
    "    okt_mode: Optional[str] = 'morphs',\n",
    "    stop_words: List[str] = None\n",
    ") -> Tuple[Any, Any, Any, Any, Any, Any]:\n",
    "    \"\"\"\n",
    "    TF-IDF ì¸ì½”ë”© (ìºì‹œ ì—†ì´ ë§¤ë²ˆ ìƒˆë¡œ ê³„ì‚°)\n",
    "    \"\"\"\n",
    "    print(\"3. Tfidf Encoding ì‹œì‘...\")\n",
    "\n",
    "    # ---------- í† í¬ë‚˜ì´ì € ì„ íƒ ----------\n",
    "    if tokenizer_name == 'okt':\n",
    "        if okt_mode not in OKT_TOKENIZER_MAP:\n",
    "            raise ValueError(\"okt_modeëŠ” 'morphs', 'nouns', 'pos' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "        base_tokenizer = OKT_TOKENIZER_MAP[okt_mode]\n",
    "        print(f\"   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Okt (Mode: {okt_mode})\")\n",
    "    elif tokenizer_name == 'simple':\n",
    "        base_tokenizer = simple_tokenizer\n",
    "        print(f\"   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Simple (ê³µë°± ë¶„ë¦¬)\")\n",
    "    else:\n",
    "        raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” í† í¬ë‚˜ì´ì €: {tokenizer_name}\")\n",
    "\n",
    "    # stop_words ë¥¼ í† í¬ë‚˜ì´ì €ì— ì „ë‹¬\n",
    "    def tokenizer_with_sw(text):\n",
    "        return base_tokenizer(text, stop_words=stop_words or [])\n",
    "\n",
    "    # ---------- TF-IDF ----------\n",
    "    tfidf_vect = TfidfVectorizer(tokenizer=tokenizer_with_sw)\n",
    "\n",
    "    X = train_df['cleaned_conversation']\n",
    "    y = train_df['label_encoded']\n",
    "\n",
    "    X_tfidf = tfidf_vect.fit_transform(X)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    X_test_final = tfidf_vect.transform(test_df['cleaned_conversation'])\n",
    "\n",
    "    print(f\"   - **íŠ¹ì§•(ì»¬ëŸ¼) ê°œìˆ˜ (Vocabulary Size): {X_tfidf.shape[1]}**\")\n",
    "    print(f\"   - í•™ìŠµìš© ë°ì´í„° shape: {X_tr.shape}\")\n",
    "    print(f\"   - ê²€ì¦ìš© ë°ì´í„° shape: {X_val.shape}\")\n",
    "\n",
    "    return X_tr, X_val, y_tr, y_val, X_test_final, tfidf_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d9dc8-20a5-4457-94f1-0a20838e1d1a",
   "metadata": {},
   "source": [
    "# 2. ML Train & F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "30d96f8d-c09d-4f0e-ba6b-18a04ef248c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_name: str, model_class: Callable, model_params: Dict[str, Any], \n",
    "                             X_tr, X_val, y_tr, y_val) -> Dict[str, Any]:\n",
    "    \"\"\"íŠ¹ì • ëª¨ë¸ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ê³  ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. XGBoost CPU/GPU ì„¤ì • í¬í•¨.\"\"\"\n",
    "    \n",
    "    if model_name.startswith('XGBoost'):\n",
    "        if 'tree_method' not in model_params:\n",
    "                # 1. tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš°: 'hist'ë¡œ ê°•ì œ ì„¤ì • (XGBoostError ë°©ì§€)\n",
    "                model_params['tree_method'] = 'hist' \n",
    "                print(\"   - XGBoost: tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ 'hist' ì‚¬ìš©.\")\n",
    "        else:\n",
    "                # 2. tree_methodê°€ ëª…ì‹œëœ ê²½ìš°: ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°’ì„ ì¶œë ¥\n",
    "                method = model_params['tree_method']\n",
    "                if method == 'gpu_hist':\n",
    "                    # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ 'gpu_hist'ë¥¼ ì„¤ì •í•˜ë©´ ì˜¤ë¥˜ ê°€ëŠ¥ì„±ì„ ì•ˆë‚´\n",
    "                    print(f\"   - XGBoost: GPU ê°€ì† ('{method}')ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (GPU ë¯¸ì»´íŒŒì¼ ì‹œ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥)\")\n",
    "                elif method == 'hist':\n",
    "                    print(f\"   - XGBoost: CPU ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬ ('{method}')ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                \n",
    "        model_params.setdefault('use_label_encoder', False)\n",
    "        model_params.setdefault('eval_metric', 'mlogloss')\n",
    "\n",
    "            \n",
    "    print(f\"\\n ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: {model_name} ì‹œì‘...\")\n",
    "\n",
    "    model = model_class(**model_params)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    f1_micro = f1_score(y_val, y_pred, average='micro')\n",
    "    CLASS_NAMES = ['í˜‘ë°• ëŒ€í™”', 'ê°ˆì·¨ ëŒ€í™”', 'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”', 'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”', 'ì¼ë°˜ ëŒ€í™”']\n",
    "    report = classification_report(y_val, y_pred,target_names=CLASS_NAMES)\n",
    "    \n",
    "    print(f\"   - F1 Micro Score: {f1_micro:.4f}\")\n",
    "    print(\"\\n[ Classification Report ]\")\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'f1_micro': f1_micro,\n",
    "        'classification_report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dc2409a5-951c-4d91-adee-9bbd0823fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_flexible(\n",
    "    data_path: str = './data/',\n",
    "    tokenizer_name: str = 'okt',\n",
    "    okt_mode: Optional[str] = 'morphs',\n",
    "    experiments_config: List[Dict[str, Any]] = None,\n",
    "    stop_words: List[str] = None\n",
    "):\n",
    "    print(\" ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ \")\n",
    "    try:\n",
    "        # 1. ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬ (í•œ ë²ˆë§Œ)\n",
    "        train_df, test_df = load_data(url=data_path)\n",
    "        train_df, test_df = preprocess_data(train_df, test_df)\n",
    "\n",
    "        # 2. TF-IDF (ìºì‹œ ì—†ì´)\n",
    "        X_tr, X_val, y_tr, y_val, X_test_final, tfidf_vect = encode_data_tfidf(\n",
    "            train_df, test_df,\n",
    "            tokenizer_name=tokenizer_name,\n",
    "            okt_mode=okt_mode,\n",
    "            stop_words=stop_words\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\në°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        import traceback, sys\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        return None, None\n",
    "\n",
    "    # ---------- ëª¨ë¸ ì‹¤í—˜ ----------\n",
    "    if experiments_config is None:\n",
    "        experiments_config = [\n",
    "            {'name': 'LR_Baseline',  'class': LogisticRegression,\n",
    "             'params': {'random_state': 24, 'solver': 'liblinear'}},\n",
    "            {'name': 'RF_Baseline',  'class': RandomForestClassifier,\n",
    "             'params': {'random_state': 24}},\n",
    "            {'name': 'XGBoost_Baseline', 'class': XGBClassifier,\n",
    "             'params': {'random_state': 24}}\n",
    "        ]\n",
    "\n",
    "    results = []\n",
    "    for exp in experiments_config:\n",
    "        result = train_and_evaluate_model(\n",
    "            exp['name'], exp['class'], exp['params'].copy(),\n",
    "            X_tr, X_val, y_tr, y_val\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    # ---------- ìš”ì•½ ----------\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\"ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (F1 Micro)\")\n",
    "    for r in results:\n",
    "        print(f\"- **{r['model_name']}**: **{r['f1_micro']:.4f}**\")\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    return results, X_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07282c-a457-4e84-8456-adeb1047869b",
   "metadata": {},
   "source": [
    "# 3. Submission ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7dd064c7-924a-4796-b307-c2ad6252a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(best_model: Any, X_test_final: Any, data_path: str = './data/', name='') -> None:\n",
    "    \n",
    "    print(\"\\nğŸš€ Test ë°ì´í„° ìµœì¢… ì¶”ë¡  ì‹œì‘...\")\n",
    "    \n",
    "    # Test ë°ì´í„° ì¶”ë¡  (ìˆ«ì ë ˆì´ë¸” 0, 1, 2, 3, 4)\n",
    "    y_test_pred_encoded = best_model.predict(X_test_final)\n",
    "    y_submission = y_test_pred_encoded # NumPy ë°°ì—´ (ì¶”ë¡  ê²°ê³¼)\n",
    "\n",
    "    # 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    base_submission_path = os.path.join(data_path, 'submission.csv')\n",
    "    \n",
    "    # 2. [í•µì‹¬ ìˆ˜ì •]: ê¸°ë³¸ submission.csv íŒŒì¼ì„ ê°•ì œë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤ (try-except ì œê±°).\n",
    "    submission_df = pd.read_csv(base_submission_path)\n",
    "    print(f\"   - ê¸°ì¡´ Submission ì–‘ì‹ íŒŒì¼ ë¡œë“œ: {base_submission_path} (Shape: {submission_df.shape})\")\n",
    "\n",
    "    # 3. 'class' ì»¬ëŸ¼ì— ì¶”ë¡  ê²°ê³¼ ë®ì–´ì“°ê¸°\n",
    "    if len(submission_df) == len(y_submission):\n",
    "        submission_df['class'] = y_submission\n",
    "    else:\n",
    "        print(f\"ê²½ê³ : ì¶”ë¡  ê²°ê³¼({len(y_submission)})ì™€ ê¸°ì¡´ ì œì¶œ íŒŒì¼ í–‰({len(submission_df)})ì˜ ê°œìˆ˜ê°€ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤. ê°•ì œë¡œ class ì»¬ëŸ¼ì„ ë®ì–´ì”ë‹ˆë‹¤.\")\n",
    "        submission_df['class'] = y_submission \n",
    "        \n",
    "    # 4. Submission íŒŒì¼ ì €ì¥ (name í¬í•¨)\n",
    "    submission_path = os.path.join(data_path, f'submission{name}.csv')\n",
    "    submission_df.to_csv(submission_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"ìµœì¢… ì¶”ë¡  ì™„ë£Œ ë° Submission íŒŒì¼ ì €ì¥: {submission_path} (Shape: {submission_df.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc5833-9f2c-495b-9cf9-ad79099f7dac",
   "metadata": {},
   "source": [
    "# 4. PipeLine & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a3de099a-71f5-47f2-97b5-6d1c3677d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1ì°¨ ì‹¤í—˜: ---\n",
      " ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ \n",
      "1. ë°ì´í„° ë¡œë“œ ì‹œì‘...\n",
      "   - í›ˆë ¨ ë°ì´í„° shape: (5048, 2)\n",
      "   - í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: (500, 1)\n",
      "2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ë ˆì´ë¸” í´ë˜ìŠ¤: 5ê°œ\n",
      "3. Tfidf Encoding ì‹œì‘...\n",
      "   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Okt (Mode: pos)\n",
      "   - **íŠ¹ì§•(ì»¬ëŸ¼) ê°œìˆ˜ (Vocabulary Size): 9552**\n",
      "   - í•™ìŠµìš© ë°ì´í„° shape: (4038, 9552)\n",
      "   - ê²€ì¦ìš© ë°ì´í„° shape: (1010, 9552)\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: LR_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8515\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.88      0.80      0.84       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.87      0.84      0.85       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.80      0.85      0.82       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.74      0.84      0.79       219\n",
      "       ì¼ë°˜ ëŒ€í™”       1.00      0.92      0.96       220\n",
      "\n",
      "    accuracy                           0.85      1010\n",
      "   macro avg       0.86      0.85      0.85      1010\n",
      "weighted avg       0.86      0.85      0.85      1010\n",
      "\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: RF_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8168\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.82      0.75      0.78       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.80      0.78      0.79       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.79      0.84      0.82       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.70      0.79      0.74       219\n",
      "       ì¼ë°˜ ëŒ€í™”       1.00      0.91      0.95       220\n",
      "\n",
      "    accuracy                           0.82      1010\n",
      "   macro avg       0.82      0.81      0.82      1010\n",
      "weighted avg       0.82      0.82      0.82      1010\n",
      "\n",
      "   - XGBoost: tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ 'hist' ì‚¬ìš©.\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: XGBoost_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8218\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.79      0.76      0.77       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.83      0.77      0.80       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.78      0.85      0.81       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.72      0.78      0.75       219\n",
      "       ì¼ë°˜ ëŒ€í™”       0.99      0.94      0.97       220\n",
      "\n",
      "    accuracy                           0.82      1010\n",
      "   macro avg       0.82      0.82      0.82      1010\n",
      "weighted avg       0.83      0.82      0.82      1010\n",
      "\n",
      "\n",
      "==============================================\n",
      "ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (F1 Micro)\n",
      "- **LR_Baseline**: **0.8515**\n",
      "- **RF_Baseline**: **0.8168**\n",
      "- **XGBoost_Baseline**: **0.8218**\n",
      "==============================================\n",
      "\n",
      "ğŸš€ Test ë°ì´í„° ìµœì¢… ì¶”ë¡  ì‹œì‘...\n",
      "   - ê¸°ì¡´ Submission ì–‘ì‹ íŒŒì¼ ë¡œë“œ: ./data/submission.csv (Shape: (500, 2))\n",
      "ìµœì¢… ì¶”ë¡  ì™„ë£Œ ë° Submission íŒŒì¼ ì €ì¥: ./data/submission_pos.csv (Shape: (500, 2))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ì‹¤í—˜ 1\n",
    "    stop_words = [\"ëŒ€ë¦¬\", \"íšŒì‚¬\", \"ê³¼ì¥\", \"ë¶€ì¥\", \"ì—…ë¬´\", \"íœ´ê°€\", \"í‡´ê·¼\", \"ìë„¤\"]\n",
    "    mode = \"pos\"\n",
    "    print(\"--- 1ì°¨ ì‹¤í—˜: ---\")\n",
    "    results1, X_test1 = run_experiment_flexible(tokenizer_name='okt', okt_mode=mode, stop_words=stop_words)\n",
    "    best1 = max(results1, key=lambda x: x['f1_micro'])\n",
    "    make_submission(best1['model'], X_test1, name='_pos')\n",
    "\n",
    "    # ì‹¤í—˜ 2:\n",
    "    # stop_words = []\n",
    "    # mode = \"pos\"\n",
    "    # print(\"--- 2ì°¨ ì‹¤í—˜: ---\")\n",
    "    # results1, X_test1 = run_experiment_flexible(tokenizer_name='okt', okt_mode=mode, stop_words=stop_words)\n",
    "    # best1 = max(results1, key=lambda x: x['f1_micro'])\n",
    "    # make_submission(best1['model'], X_test1, name='_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc380ee-220e-4d3c-a0fc-f9c794da9a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44e01dca-f596-4d9d-9a02-d85483273efc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ì´ì „ ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6ac3626a-2c9c-4629-b123-ccc9dfb80076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1ì°¨ ì‹¤í—˜: Okt  ì‚¬ìš© ---\n",
      " ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ \n",
      "1. ë°ì´í„° ë¡œë“œ ì‹œì‘...\n",
      "   - í›ˆë ¨ ë°ì´í„° shape: (5048, 2)\n",
      "   - í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: (500, 1)\n",
      "2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ë ˆì´ë¸” í´ë˜ìŠ¤: 5ê°œ\n",
      "3. Tfidf Encoding ì‹œì‘...\n",
      "   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Okt (Mode: pos)\n",
      "   - **íŠ¹ì§•(ì»¬ëŸ¼) ê°œìˆ˜ (Vocabulary Size): 9560**\n",
      "   - í•™ìŠµìš© ë°ì´í„° shape: (4038, 9560)\n",
      "   - ê²€ì¦ìš© ë°ì´í„° shape: (1010, 9560)\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: LR_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8653\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.88      0.81      0.84       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.87      0.83      0.85       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.87      0.87      0.87       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.75      0.88      0.81       219\n",
      "       ì¼ë°˜ ëŒ€í™”       1.00      0.93      0.96       220\n",
      "\n",
      "    accuracy                           0.87      1010\n",
      "   macro avg       0.87      0.86      0.87      1010\n",
      "weighted avg       0.87      0.87      0.87      1010\n",
      "\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: RF_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8396\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.85      0.77      0.80       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.83      0.79      0.81       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.85      0.87      0.86       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.71      0.85      0.78       219\n",
      "       ì¼ë°˜ ëŒ€í™”       1.00      0.91      0.95       220\n",
      "\n",
      "    accuracy                           0.84      1010\n",
      "   macro avg       0.85      0.84      0.84      1010\n",
      "weighted avg       0.85      0.84      0.84      1010\n",
      "\n",
      "   - XGBoost: tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ 'hist' ì‚¬ìš©.\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: XGBoost_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8386\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.83      0.74      0.78       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.83      0.78      0.80       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.87      0.86      0.86       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.71      0.84      0.77       219\n",
      "       ì¼ë°˜ ëŒ€í™”       0.99      0.95      0.97       220\n",
      "\n",
      "    accuracy                           0.84      1010\n",
      "   macro avg       0.84      0.83      0.84      1010\n",
      "weighted avg       0.85      0.84      0.84      1010\n",
      "\n",
      "\n",
      "==============================================\n",
      "ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (F1 Micro)\n",
      "- **LR_Baseline**: **0.8653**\n",
      "- **RF_Baseline**: **0.8396**\n",
      "- **XGBoost_Baseline**: **0.8386**\n",
      "\n",
      "==============================================\n",
      "ìµœì  ëª¨ë¸ ì„ ì •: LR_Baseline (F1 Micro: 0.8653)\n",
      "\n",
      "ğŸš€ Test ë°ì´í„° ìµœì¢… ì¶”ë¡  ì‹œì‘...\n",
      "   - ê¸°ì¡´ Submission ì–‘ì‹ íŒŒì¼ ë¡œë“œ: ./data/submission.csv (Shape: (500, 2))\n",
      "ìµœì¢… ì¶”ë¡  ì™„ë£Œ ë° Submission íŒŒì¼ ì €ì¥: ./data/submission_baseline.csv (Shape: (500, 2))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ğŸ§ª ì‹¤í—˜ 1: Okt (morphs) ì‚¬ìš© - ê¸°ë³¸ ì„¤ì •\n",
    "    print(\"--- 1ì°¨ ì‹¤í—˜: Okt  ì‚¬ìš© ---\")\n",
    "    results_morphs, X_test_final_morphs = run_experiment_flexible( \n",
    "        tokenizer_name='okt',\n",
    "        okt_mode='pos'\n",
    "    )\n",
    "\n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "    best_result = max(results_morphs, key=lambda x: x['f1_micro'])\n",
    "    best_model = best_result['model']\n",
    "    best_model_name = best_result['model_name']\n",
    "    \n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"ìµœì  ëª¨ë¸ ì„ ì •: {best_model_name} (F1 Micro: {best_result['f1_micro']:.4f})\")\n",
    "    \n",
    "    # 2. make_submission í˜¸ì¶œ\n",
    "    make_submission(\n",
    "        best_model=best_model, \n",
    "        X_test_final=X_test_final_morphs, \n",
    "        #data_path=data_path,\n",
    "        name='_baseline'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4c3ebd01-7108-4003-81d7-6a7c88e1104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2ì°¨ ì‹¤í—˜: Okt (nouns) ì‚¬ìš© ---\n",
      " ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ \n",
      "1. ë°ì´í„° ë¡œë“œ ì‹œì‘...\n",
      "   - í›ˆë ¨ ë°ì´í„° shape: (5048, 2)\n",
      "   - í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: (500, 1)\n",
      "2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ë ˆì´ë¸” í´ë˜ìŠ¤: 5ê°œ\n",
      "3. Tfidf Encoding ì‹œì‘...\n",
      "   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Okt (Mode: nouns)\n",
      "   - **íŠ¹ì§•(ì»¬ëŸ¼) ê°œìˆ˜ (Vocabulary Size): 7867**\n",
      "   - í•™ìŠµìš© ë°ì´í„° shape: (4038, 7867)\n",
      "   - ê²€ì¦ìš© ë°ì´í„° shape: (1010, 7867)\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: LR_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8099\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.75      0.63      0.68       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.76      0.74      0.75       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.87      0.87      0.87       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.69      0.85      0.76       219\n",
      "       ì¼ë°˜ ëŒ€í™”       1.00      0.92      0.96       220\n",
      "\n",
      "    accuracy                           0.81      1010\n",
      "   macro avg       0.81      0.80      0.81      1010\n",
      "weighted avg       0.82      0.81      0.81      1010\n",
      "\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: RF_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.7782\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.71      0.57      0.63       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.72      0.67      0.69       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.87      0.85      0.86       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.63      0.84      0.72       219\n",
      "       ì¼ë°˜ ëŒ€í™”       1.00      0.92      0.96       220\n",
      "\n",
      "    accuracy                           0.78      1010\n",
      "   macro avg       0.79      0.77      0.77      1010\n",
      "weighted avg       0.79      0.78      0.78      1010\n",
      "\n",
      "   - XGBoost: tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ 'hist' ì‚¬ìš©.\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: XGBoost_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.7950\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.70      0.63      0.66       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.78      0.70      0.74       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.85      0.84      0.85       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.67      0.84      0.75       219\n",
      "       ì¼ë°˜ ëŒ€í™”       0.99      0.93      0.96       220\n",
      "\n",
      "    accuracy                           0.80      1010\n",
      "   macro avg       0.80      0.79      0.79      1010\n",
      "weighted avg       0.80      0.80      0.80      1010\n",
      "\n",
      "\n",
      "==============================================\n",
      "ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (F1 Micro)\n",
      "- **LR_Baseline**: **0.8099**\n",
      "- **RF_Baseline**: **0.7782**\n",
      "- **XGBoost_Baseline**: **0.7950**\n",
      "\n",
      "==============================================\n",
      "ìµœì  ëª¨ë¸ ì„ ì •: LR_Baseline (F1 Micro: 0.8099)\n",
      "\n",
      "ğŸš€ Test ë°ì´í„° ìµœì¢… ì¶”ë¡  ì‹œì‘...\n",
      "   - ê¸°ì¡´ Submission ì–‘ì‹ íŒŒì¼ ë¡œë“œ: ./data/submission.csv (Shape: (500, 2))\n",
      "ìµœì¢… ì¶”ë¡  ì™„ë£Œ ë° Submission íŒŒì¼ ì €ì¥: ./data/submission_baseline_nouns.csv (Shape: (500, 2))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ğŸ§ª ì‹¤í—˜ 2: Okt (nouns) ì‚¬ìš© - ê¸°ë³¸ ì„¤ì •\n",
    "    print(\"--- 2ì°¨ ì‹¤í—˜: Okt (nouns) ì‚¬ìš© ---\")\n",
    "    results_morphs, X_test_final_morphs = run_experiment_flexible( \n",
    "        tokenizer_name='okt',\n",
    "        okt_mode='nouns'\n",
    "    )\n",
    "\n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "    best_result = max(results_morphs, key=lambda x: x['f1_micro'])\n",
    "    best_model = best_result['model']\n",
    "    best_model_name = best_result['model_name']\n",
    "    \n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"ìµœì  ëª¨ë¸ ì„ ì •: {best_model_name} (F1 Micro: {best_result['f1_micro']:.4f})\")\n",
    "    \n",
    "    # 2. make_submission í˜¸ì¶œ\n",
    "    make_submission(\n",
    "        best_model=best_model, \n",
    "        X_test_final=X_test_final_morphs, \n",
    "        #data_path=data_path,\n",
    "        name='_baseline_nouns'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "00b080b9-bcb9-47e5-aa90-25c802fde6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3ì°¨ ì‹¤í—˜: Okt (pos) ì‚¬ìš© ---\n",
      " ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ \n",
      "1. ë°ì´í„° ë¡œë“œ ì‹œì‘...\n",
      "   - í›ˆë ¨ ë°ì´í„° shape: (5048, 2)\n",
      "   - í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: (500, 1)\n",
      "2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ë ˆì´ë¸” í´ë˜ìŠ¤: 5ê°œ\n",
      "3. Tfidf Encoding ì‹œì‘...\n",
      "   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Okt (Mode: morphs)\n",
      "   - **íŠ¹ì§•(ì»¬ëŸ¼) ê°œìˆ˜ (Vocabulary Size): 10110**\n",
      "   - í•™ìŠµìš© ë°ì´í„° shape: (4038, 10110)\n",
      "   - ê²€ì¦ìš© ë°ì´í„° shape: (1010, 10110)\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: LR_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8733\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.87      0.82      0.84       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.87      0.86      0.87       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.89      0.87      0.88       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.76      0.88      0.82       219\n",
      "       ì¼ë°˜ ëŒ€í™”       1.00      0.93      0.96       220\n",
      "\n",
      "    accuracy                           0.87      1010\n",
      "   macro avg       0.88      0.87      0.87      1010\n",
      "weighted avg       0.88      0.87      0.87      1010\n",
      "\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: RF_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8337\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.80      0.76      0.78       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.84      0.77      0.80       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.86      0.87      0.86       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.71      0.85      0.77       219\n",
      "       ì¼ë°˜ ëŒ€í™”       1.00      0.91      0.95       220\n",
      "\n",
      "    accuracy                           0.83      1010\n",
      "   macro avg       0.84      0.83      0.83      1010\n",
      "weighted avg       0.84      0.83      0.84      1010\n",
      "\n",
      "   - XGBoost: tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ 'hist' ì‚¬ìš©.\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: XGBoost_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8386\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”       0.83      0.73      0.77       179\n",
      "       ê°ˆì·¨ ëŒ€í™”       0.82      0.80      0.81       196\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.88      0.86      0.87       196\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.71      0.84      0.77       219\n",
      "       ì¼ë°˜ ëŒ€í™”       0.99      0.95      0.97       220\n",
      "\n",
      "    accuracy                           0.84      1010\n",
      "   macro avg       0.84      0.83      0.84      1010\n",
      "weighted avg       0.85      0.84      0.84      1010\n",
      "\n",
      "\n",
      "==============================================\n",
      "ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (F1 Micro)\n",
      "- **LR_Baseline**: **0.8733**\n",
      "- **RF_Baseline**: **0.8337**\n",
      "- **XGBoost_Baseline**: **0.8386**\n",
      "\n",
      "==============================================\n",
      "ìµœì  ëª¨ë¸ ì„ ì •: LR_Baseline (F1 Micro: 0.8733)\n",
      "\n",
      "ğŸš€ Test ë°ì´í„° ìµœì¢… ì¶”ë¡  ì‹œì‘...\n",
      "   - ê¸°ì¡´ Submission ì–‘ì‹ íŒŒì¼ ë¡œë“œ: ./data/submission.csv (Shape: (500, 2))\n",
      "ìµœì¢… ì¶”ë¡  ì™„ë£Œ ë° Submission íŒŒì¼ ì €ì¥: ./data/submission_baseline_pos.csv (Shape: (500, 2))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ğŸ§ª ì‹¤í—˜ 2: Okt (pos) ì‚¬ìš© - ê¸°ë³¸ ì„¤ì •\n",
    "    print(\"--- 3ì°¨ ì‹¤í—˜: Okt (pos) ì‚¬ìš© ---\")\n",
    "    results_morphs, X_test_final_morphs = run_experiment_flexible( \n",
    "        tokenizer_name='okt',\n",
    "        okt_mode='morphs'\n",
    "    )\n",
    "\n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "    best_result = max(results_morphs, key=lambda x: x['f1_micro'])\n",
    "    best_model = best_result['model']\n",
    "    best_model_name = best_result['model_name']\n",
    "    \n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"ìµœì  ëª¨ë¸ ì„ ì •: {best_model_name} (F1 Micro: {best_result['f1_micro']:.4f})\")\n",
    "    \n",
    "    # 2. make_submission í˜¸ì¶œ\n",
    "    make_submission(\n",
    "        best_model=best_model, \n",
    "        X_test_final=X_test_final_morphs, \n",
    "        #data_path=data_path,\n",
    "        name='_baseline_pos'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d327e8a-124f-4ff3-a1eb-87556b52f07e",
   "metadata": {},
   "source": [
    "# íšŒê³ ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b90f9-d6d8-4b52-a3a7-463df471630c",
   "metadata": {},
   "source": [
    "- ì¼ë°˜ baselindì—ì„œ Okt ë³€í™”\n",
    "    - ì•„ë¬´ë˜ë„ ê·¸ëƒ¥ ëª…ì‚¬ë§Œ ë½‘ëƒ, ê·¸ ì™¸ ì „ë¶€ ì‚¬ìš©í•˜ëƒì— ë”°ë¼ì„œ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆëŠ” ë“¯ í•˜ë‹¤.\n",
    "    - ëª…ì‚¬ë§Œ ë½‘ê²Œ ë  ê²½ìš°, ì‹¤ì œ í–‰ìœ„ë‚˜ ê·¸ëŸ° ë™ì‘ë“¤ì„ ì•Œ ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì— ê·¸ëŸ° ë“¯ í•˜ë‹¤.\n",
    "        - morphs : ìµœì  ëª¨ë¸ ì„ ì •: LR_Baseline (F1 Micro: 0.8881)\n",
    "        - nouns : ìµœì  ëª¨ë¸ ì„ ì •: LR_Baseline (F1 Micro: 0.8446)\n",
    "        - pos: ìµœì  ëª¨ë¸ ì„ ì •: LR_Baseline (F1 Micro: 0.8653)\n",
    "            - {\"Noun\", \"Verb\", \"Adjective\", \"Exclamation\", \"Adverb\"}ë§Œ ì¶”ì¶œí•´ë´„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fbba0-4992-4bf2-8c84-49bbdf247219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

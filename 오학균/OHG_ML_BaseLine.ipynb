{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5be008-8211-4595-a348-d1fec62dcc9b",
   "metadata": {},
   "source": [
    "# 0. Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8895ee-0b27-4b42-bb16-b968dc11e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import requests \n",
    "import zipfile \n",
    "import io \n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from typing import Dict, Any, Callable, Tuple, List, Optional\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0173027-4811-4df4-977d-cb2f0837153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url: str = 'data/') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê²°í•©í•©ë‹ˆë‹¤. íŒŒì¼ì´ ì—†ìœ¼ë©´ GitHubì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"1. ë°ì´í„° ë¡œë“œ ì‹œì‘...\")\n",
    "    \n",
    "    # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    # íŒŒì¼ ì´ë¦„ì€ ìˆ˜ì •í•  ê²ƒ\n",
    "    required_files = ['train.csv', 'general_dialog1.csv', 'general_dialog2.csv', 'test.json']\n",
    "    all_files_exist = all(os.path.exists(os.path.join(url, f)) for f in required_files)\n",
    "\n",
    "    if not all_files_exist:\n",
    "        print(\"í•„ìˆ˜ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. GitHubì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "        download_url = \"https://github.com/tunib-ai/DKTC/archive/refs/heads/main.zip\"\n",
    "        \n",
    "        try:\n",
    "            # data ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "            os.makedirs(url, exist_ok=True)\n",
    "            \n",
    "            print(\"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "            response = requests.get(download_url)\n",
    "            response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ\n",
    "\n",
    "            print(\"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, ì••ì¶• í•´ì œ ì¤‘...\")\n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                for file in z.namelist():\n",
    "                    # DKTC-main/data/ í´ë” ì•ˆ íŒŒì¼ë§Œ ì¶”ì¶œ\n",
    "                    if file.startswith(\"DKTC-main/data/\") and not file.endswith('/'):\n",
    "                        # íŒŒì¼ ê²½ë¡œ ì¬ì„¤ì •: 'DKTC-main/data/íŒŒì¼ëª…' -> 'data/íŒŒì¼ëª…'\n",
    "                        target_path = os.path.join(url, os.path.relpath(file, \"DKTC-main/data\"))\n",
    "                        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "                        with z.open(file) as source, open(target_path, \"wb\") as target:\n",
    "                            target.write(source.read())\n",
    "            print(f\"'{url}' í´ë”ì— ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP ì˜¤ë¥˜ ë˜ëŠ” ì—°ê²° ë¬¸ì œ. {e}\")\n",
    "            raise FileNotFoundError(\"ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.\")\n",
    "        except Exception as e:\n",
    "            print(f\"íŒŒì¼ ì••ì¶• í•´ì œ/ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            raise FileNotFoundError(\"ë‹¤ìš´ë¡œë“œ í›„ íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "    # --- ë°ì´í„° ë¡œë“œ ì§„í–‰ (íŒŒì¼ì´ ì¡´ì¬í•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œ í›„) ---\n",
    "    train = pd.read_csv(os.path.join(url, 'train.csv'))\n",
    "    a = pd.read_csv(os.path.join(url, 'general_dialog1.csv')).rename(columns={'dialogue': 'conversation'})\n",
    "    a['class'] = 'ì¼ë°˜ ëŒ€í™”'\n",
    "    a['idx'] = range(0,len(a))\n",
    "    a = a[['idx','class','conversation']]\n",
    "    b = pd.read_csv(os.path.join(url, 'general_dialog2.csv'))\n",
    "    b = b[b['class'] == \"ì¼ë°˜ ëŒ€í™”\"] \n",
    "    train_df = pd.concat([train, a, b], axis=0, ignore_index=True).drop(columns='idx', errors='ignore')\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (test.json)\n",
    "    with open(os.path.join(url, \"test.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        test_df = pd.DataFrame({\"conversation\": [v[\"text\"] for v in json.load(f).values()]})\n",
    "    \n",
    "    print(f\"   - í›ˆë ¨ ë°ì´í„° shape: {train_df.shape}\")\n",
    "    print(f\"   - í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {test_df.shape}\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc48af-4e16-4945-ad3a-1d423f104ba8",
   "metadata": {},
   "source": [
    "# 1. Preprocessing & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26555d13-7dee-4cb9-bb93-fd23faadd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[int, str]]:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ìš”ì²­ëœ ê³ ì •ëœ ìˆ«ì ë ˆì´ë¸”ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í•¨ìˆ˜: íŠ¹ìˆ˜ ë¬¸ì ë° ë‹¤ì¤‘ ê³µë°± ì œê±° (ìƒëµ)\n",
    "    def clean_text(text: str) -> str:\n",
    "        text = str(text) \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE) \n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text \n",
    "\n",
    "    train_df['cleaned_conversation'] = train_df['conversation'].apply(clean_text)\n",
    "    test_df['cleaned_conversation'] = test_df['conversation'].apply(clean_text)\n",
    "\n",
    "    LABEL_MAPPING = {\n",
    "        'í˜‘ë°• ëŒ€í™”': 0,\n",
    "        'ê°ˆì·¨ ëŒ€í™”': 1,\n",
    "        'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”': 2,\n",
    "        'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”': 3,\n",
    "        'ì¼ë°˜ ëŒ€í™”': 4\n",
    "    }\n",
    "    \n",
    "    # ì¸ì½”ë”© ìˆ˜í–‰\n",
    "    train_df['label_encoded'] = train_df['class'].map(LABEL_MAPPING)\n",
    "    \n",
    "    print(f\"   - ë ˆì´ë¸” í´ë˜ìŠ¤: {len(LABEL_MAPPING)}ê°œ\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07de5711-cf0a-46b1-8adc-f2ae6f5e74b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: Attempt to protect stack guard pages failed.\n",
      "OpenJDK 64-Bit Server VM warning: Attempt to deallocate stack guard pages failed.\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "def get_okt_tokenizer(mode: str = 'morphs') -> Callable[[str], List[str]]:\n",
    "    \"\"\"Okt ê°ì²´ì™€ ëª¨ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í† í¬ë‚˜ì´ì € í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if mode == 'morphs':\n",
    "        def okt_tokenizer(text: str) -> list:\n",
    "            if pd.isna(text): return []\n",
    "            return okt.morphs(text) # í˜•íƒœì†Œ ì „ì²´ ì‚¬ìš©\n",
    "    elif mode == 'nouns':\n",
    "        def okt_tokenizer(text: str) -> list:\n",
    "            if pd.isna(text): return []\n",
    "            return okt.nouns(text)  # ëª…ì‚¬ë§Œ ì‚¬ìš©\n",
    "    else:\n",
    "        raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” Okt ëª¨ë“œ: {mode}. 'morphs' ë˜ëŠ” 'nouns'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "        \n",
    "    return okt_tokenizer\n",
    "\n",
    "def simple_tokenizer(text: str) -> list:\n",
    "    \"\"\"ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜ í† í¬ë‚˜ì´ì € (ì˜ˆì‹œ)\"\"\"\n",
    "    return text.split() if isinstance(text, str) else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c727cfb-4945-404a-b77b-ccf0c29e92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_tfidf(train_df: pd.DataFrame, test_df: pd.DataFrame, \n",
    "                      tokenizer_name: str, okt_mode: Optional[str] = 'morphs') -> Tuple[Any, Any, Any, Any, Any, TfidfVectorizer]:\n",
    "    \"\"\"TfidfVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"3. Tfidf Encoding ì‹œì‘...\")\n",
    "    \n",
    "    if tokenizer_name == 'okt':\n",
    "        tokenizer = get_okt_tokenizer(mode=okt_mode)\n",
    "        print(f\"   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Okt (Mode: {okt_mode})\")\n",
    "    elif tokenizer_name == 'simple':\n",
    "        tokenizer = simple_tokenizer\n",
    "        print(f\"   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Simple (ê³µë°± ë¶„ë¦¬)\")\n",
    "    else:\n",
    "        raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” í† í¬ë‚˜ì´ì €: {tokenizer_name}\")\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(tokenizer=tokenizer)\n",
    "    \n",
    "    X = train_df['cleaned_conversation']\n",
    "    y = train_df['label_encoded']\n",
    "    \n",
    "    X_tfidf = tfidf_vect.fit_transform(X)\n",
    "    \n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_test_final = tfidf_vect.transform(test_df['cleaned_conversation'])\n",
    "    \n",
    "    print(f\"   - **íŠ¹ì§•(ì»¬ëŸ¼) ê°œìˆ˜ (Vocabulary Size): {X_tfidf.shape[1]}**\")\n",
    "    print(f\"   - í•™ìŠµìš© ë°ì´í„° shape: {X_tr.shape}\")\n",
    "    print(f\"   - ê²€ì¦ìš© ë°ì´í„° shape: {X_val.shape}\")\n",
    "    \n",
    "    return X_tr, X_val, y_tr, y_val, X_test_final, tfidf_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d9dc8-20a5-4457-94f1-0a20838e1d1a",
   "metadata": {},
   "source": [
    "# 2. ML Train & F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d96f8d-c09d-4f0e-ba6b-18a04ef248c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_name: str, model_class: Callable, model_params: Dict[str, Any], \n",
    "                             X_tr, X_val, y_tr, y_val) -> Dict[str, Any]:\n",
    "    \"\"\"íŠ¹ì • ëª¨ë¸ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ê³  ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. XGBoost CPU/GPU ì„¤ì • í¬í•¨.\"\"\"\n",
    "    \n",
    "    if model_name.startswith('XGBoost'):\n",
    "        if 'tree_method' not in model_params:\n",
    "                # 1. tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš°: 'hist'ë¡œ ê°•ì œ ì„¤ì • (XGBoostError ë°©ì§€)\n",
    "                model_params['tree_method'] = 'hist' \n",
    "                print(\"   - XGBoost: tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ 'hist' ì‚¬ìš©.\")\n",
    "        else:\n",
    "                # 2. tree_methodê°€ ëª…ì‹œëœ ê²½ìš°: ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°’ì„ ì¶œë ¥\n",
    "                method = model_params['tree_method']\n",
    "                if method == 'gpu_hist':\n",
    "                    # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ 'gpu_hist'ë¥¼ ì„¤ì •í•˜ë©´ ì˜¤ë¥˜ ê°€ëŠ¥ì„±ì„ ì•ˆë‚´\n",
    "                    print(f\"   - XGBoost: GPU ê°€ì† ('{method}')ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (GPU ë¯¸ì»´íŒŒì¼ ì‹œ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥)\")\n",
    "                elif method == 'hist':\n",
    "                    print(f\"   - XGBoost: CPU ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬ ('{method}')ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                \n",
    "        model_params.setdefault('use_label_encoder', False)\n",
    "        model_params.setdefault('eval_metric', 'mlogloss')\n",
    "\n",
    "            \n",
    "    print(f\"\\n ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: {model_name} ì‹œì‘...\")\n",
    "\n",
    "    model = model_class(**model_params)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    f1_micro = f1_score(y_val, y_pred, average='micro')\n",
    "    report = classification_report(y_val, y_pred)\n",
    "    \n",
    "    print(f\"   - F1 Micro Score: {f1_micro:.4f}\")\n",
    "    print(\"\\n[ Classification Report ]\")\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'f1_micro': f1_micro,\n",
    "        'classification_report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2409a5-951c-4d91-adee-9bbd0823fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_flexible(data_path: str = './data/', \n",
    "                            tokenizer_name: str = 'okt',\n",
    "                            okt_mode: Optional[str] = 'morphs',\n",
    "                            experiments_config: List[Dict[str, Any]] = None):\n",
    "    \"\"\"ì „ì²´ ML íŒŒì´í”„ë¼ì¸ì„ ìœ ì—°í•œ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    print(\" ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ \")\n",
    "    \n",
    "    try:\n",
    "        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "        train_df, test_df = load_data(url=data_path)\n",
    "        train_df, test_df = preprocess_data(train_df, test_df)\n",
    "        \n",
    "        # Tfidf ì¸ì½”ë”©\n",
    "        X_tr, X_val, y_tr, y_val, X_test_final, tfidf_vect = encode_data_tfidf(\n",
    "            train_df, test_df, tokenizer_name, okt_mode\n",
    "        )\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"\\n ë°ì´í„° ë¡œë“œ/ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return\n",
    "        \n",
    "    # ê¸°ë³¸ ì‹¤í—˜ ì„¤ì •\n",
    "    if experiments_config is None:\n",
    "        experiments_config = [\n",
    "            # BaseLineì´ë¼ random_state ì œì™¸í•œ ë‚˜ë¨¸ì§€ ê°’ì€ ì „ë¶€ defaultë¡œ ì§„í–‰\n",
    "            # ë‹¤ë§Œ LRì— ê²½ìš°, ê¸°ì¡´ì—ëŠ” ì´ì§„ë¶„ë¥˜ ëª¨ë¸ì´ê¸°ì— íŒŒë¼ë¯¸í„°ë¡œ solverë¥¼ liblinearë¡œ ë³€ê²½í•˜ì—¬ ë‹¤ì¤‘ë¶„ë¥˜ë¡œ ë³€ê²½\n",
    "            {'name': 'LR_Baseline', 'class': LogisticRegression, 'params': {'random_state': 24, 'solver': 'liblinear'}},\n",
    "            {'name': 'RF_Baseline', 'class': RandomForestClassifier, 'params': {'random_state': 24}},\n",
    "            {'name': 'XGBoost_Baseline', 'class': XGBClassifier, 'params': {'random_state': 24}} \n",
    "        ]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # 3. ëª¨ë¸ë³„ í•™ìŠµ ë° í‰ê°€ ì‹¤í–‰\n",
    "    for exp in experiments_config:\n",
    "        result = train_and_evaluate_model(\n",
    "            exp['name'], exp['class'], exp['params'].copy(), X_tr, X_val, y_tr, y_val\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    # 4. ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\"ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (F1 Micro)\")\n",
    "    for res in results:\n",
    "        print(f\"- **{res['model_name']}**: **{res['f1_micro']:.4f}**\")\n",
    "        \n",
    "    return results, X_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc5833-9f2c-495b-9cf9-ad79099f7dac",
   "metadata": {},
   "source": [
    "# 3. PipeLine & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac3626a-2c9c-4629-b123-ccc9dfb80076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1ì°¨ ì‹¤í—˜: Okt (morphs) ì‚¬ìš© ---\n",
      " ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ \n",
      "1. ë°ì´í„° ë¡œë“œ ì‹œì‘...\n",
      "   - í›ˆë ¨ ë°ì´í„° shape: (5048, 2)\n",
      "   - í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: (500, 1)\n",
      "2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ë ˆì´ë¸” í´ë˜ìŠ¤: 5ê°œ\n",
      "3. Tfidf Encoding ì‹œì‘...\n",
      "   - Tfidf í† í¬ë‚˜ì´ì € ì‚¬ìš©: Okt (Mode: morphs)\n",
      "   - **íŠ¹ì§•(ì»¬ëŸ¼) ê°œìˆ˜ (Vocabulary Size): 27863**\n",
      "   - í•™ìŠµìš© ë°ì´í„° shape: (4038, 27863)\n",
      "   - ê²€ì¦ìš© ë°ì´í„° shape: (1010, 27863)\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: LR_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8733\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.78      0.81       179\n",
      "           1       0.88      0.79      0.83       196\n",
      "           2       0.90      0.89      0.89       196\n",
      "           3       0.76      0.89      0.82       219\n",
      "           4       1.00      1.00      1.00       220\n",
      "\n",
      "    accuracy                           0.87      1010\n",
      "   macro avg       0.88      0.87      0.87      1010\n",
      "weighted avg       0.88      0.87      0.87      1010\n",
      "\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: RF_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8396\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.65      0.75       179\n",
      "           1       0.84      0.78      0.81       196\n",
      "           2       0.83      0.91      0.87       196\n",
      "           3       0.70      0.85      0.77       219\n",
      "           4       1.00      0.97      0.98       220\n",
      "\n",
      "    accuracy                           0.84      1010\n",
      "   macro avg       0.85      0.83      0.84      1010\n",
      "weighted avg       0.85      0.84      0.84      1010\n",
      "\n",
      "   - XGBoost: tree_methodê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ 'hist' ì‚¬ìš©.\n",
      "\n",
      " ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: XGBoost_Baseline ì‹œì‘...\n",
      "   - F1 Micro Score: 0.8485\n",
      "\n",
      "[ Classification Report ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.75       179\n",
      "           1       0.82      0.77      0.79       196\n",
      "           2       0.91      0.88      0.89       196\n",
      "           3       0.73      0.87      0.79       219\n",
      "           4       1.00      1.00      1.00       220\n",
      "\n",
      "    accuracy                           0.85      1010\n",
      "   macro avg       0.85      0.84      0.84      1010\n",
      "weighted avg       0.85      0.85      0.85      1010\n",
      "\n",
      "\n",
      "==============================================\n",
      "ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (F1 Micro)\n",
      "- **LR_Baseline**: **0.8733**\n",
      "- **RF_Baseline**: **0.8396**\n",
      "- **XGBoost_Baseline**: **0.8485**\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ğŸ§ª ì‹¤í—˜ 1: Okt (morphs) ì‚¬ìš© - ê¸°ë³¸ ì„¤ì •\n",
    "    print(\"--- 1ì°¨ ì‹¤í—˜: Okt (morphs) ì‚¬ìš© ---\")\n",
    "    results_morphs, X_test_final_morphs = run_experiment_flexible( \n",
    "        tokenizer_name='okt',\n",
    "        okt_mode='morphs'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6772bebb-c8aa-4942-bac3-0772eafa3bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(best_model: Any, X_test_final: Any, data_path: str = './data/', name='') -> None:\n",
    "    \n",
    "    print(\"\\nğŸš€ Test ë°ì´í„° ìµœì¢… ì¶”ë¡  ì‹œì‘...\")\n",
    "    \n",
    "    # Test ë°ì´í„° ì¶”ë¡  (ìˆ«ì ë ˆì´ë¸” 0, 1, 2, 3, 4)\n",
    "    y_test_pred_encoded = best_model.predict(X_test_final)\n",
    "    y_submission = y_test_pred_encoded # NumPy ë°°ì—´ (ì¶”ë¡  ê²°ê³¼)\n",
    "\n",
    "    # 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    base_submission_path = os.path.join(data_path, 'submission.csv')\n",
    "    \n",
    "    # 2. ğŸ’¡ [í•µì‹¬ ìˆ˜ì •]: ê¸°ë³¸ submission.csv íŒŒì¼ì„ ê°•ì œë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤ (try-except ì œê±°).\n",
    "    submission_df = pd.read_csv(base_submission_path)\n",
    "    print(f\"   - ê¸°ì¡´ Submission ì–‘ì‹ íŒŒì¼ ë¡œë“œ: {base_submission_path} (Shape: {submission_df.shape})\")\n",
    "\n",
    "    # 3. 'class' ì»¬ëŸ¼ì— ì¶”ë¡  ê²°ê³¼ ë®ì–´ì“°ê¸°\n",
    "    if len(submission_df) == len(y_submission):\n",
    "        # ê¸°ì¡´ DataFrameì˜ 'class' ì»¬ëŸ¼ì„ ì¶”ë¡  ê²°ê³¼ë¡œ ë®ì–´ì”ë‹ˆë‹¤.\n",
    "        submission_df['class'] = y_submission\n",
    "    else:\n",
    "        # í–‰ ê°œìˆ˜ê°€ ë¶ˆì¼ì¹˜í•´ë„ ê°•ì œë¡œ ë®ì–´ì“°ê±°ë‚˜, ì‚¬ìš©ìì—ê²Œ ê²½ê³  í›„ ìƒˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        # ê¸°ì¡´ ì–‘ì‹ ìœ ì§€ë¥¼ ìœ„í•´ ê²½ê³  í›„ ê°•ì œ ë®ì–´ì“°ê¸°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "        print(f\"ğŸš¨ ê²½ê³ : ì¶”ë¡  ê²°ê³¼({len(y_submission)})ì™€ ê¸°ì¡´ ì œì¶œ íŒŒì¼ í–‰({len(submission_df)})ì˜ ê°œìˆ˜ê°€ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤. ê°•ì œë¡œ class ì»¬ëŸ¼ì„ ë®ì–´ì”ë‹ˆë‹¤.\")\n",
    "        # ë°ì´í„° ê¸¸ì´ ë¶ˆì¼ì¹˜ ì‹œ Pandasê°€ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ë¯€ë¡œ, ì•ˆì „í•˜ê²Œ ìƒˆ DataFrameì„ ìƒì„±í•˜ì—¬ í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "        # (ê¸°ì¡´ ì»¬ëŸ¼ ìˆ˜ë¥¼ ë§ì¶”ëŠ” ê²ƒì´ ë” ì•ˆì „í•©ë‹ˆë‹¤.)\n",
    "        submission_df['class'] = y_submission \n",
    "        \n",
    "    # 4. Submission íŒŒì¼ ì €ì¥ (name í¬í•¨)\n",
    "    submission_path = os.path.join(data_path, f'submission{name}.csv')\n",
    "    submission_df.to_csv(submission_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"ğŸ‰ ìµœì¢… ì¶”ë¡  ì™„ë£Œ ë° Submission íŒŒì¼ ì €ì¥: **{submission_path}** (Shape: {submission_df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c3ebd01-7108-4003-81d7-6a7c88e1104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================\n",
      "ğŸ¥‡ ìµœì  ëª¨ë¸ ì„ ì •: **LR_Baseline** (F1 Micro: 0.8733)\n",
      "\n",
      "ğŸš€ Test ë°ì´í„° ìµœì¢… ì¶”ë¡  ì‹œì‘...\n",
      "   - ê¸°ì¡´ Submission ì–‘ì‹ íŒŒì¼ ë¡œë“œ: ./data/submission.csv (Shape: (500, 2))\n",
      "ğŸ‰ ìµœì¢… ì¶”ë¡  ì™„ë£Œ ë° Submission íŒŒì¼ ì €ì¥: **./data/submission_baseline.csv** (Shape: (500, 2))\n"
     ]
    }
   ],
   "source": [
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "best_result = max(results_morphs, key=lambda x: x['f1_micro'])\n",
    "best_model = best_result['model']\n",
    "best_model_name = best_result['model_name']\n",
    "\n",
    "print(\"\\n==============================================\")\n",
    "print(f\"ğŸ¥‡ ìµœì  ëª¨ë¸ ì„ ì •: **{best_model_name}** (F1 Micro: {best_result['f1_micro']:.4f})\")\n",
    "\n",
    "# 2. make_submission í˜¸ì¶œ\n",
    "make_submission(\n",
    "    best_model=best_model, \n",
    "    X_test_final=X_test_final_morphs, \n",
    "    #data_path=data_path,\n",
    "    name='_baseline'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

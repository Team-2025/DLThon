{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc49a7c7-e462-4b27-8d2b-36557e739b64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 사전 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae58d0-9c14-4c0d-9dd2-ed091fb7454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "!pip install konlpy\n",
    "!pip install torch\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y fonts-nanum fonts-noto-cjk\n",
    "!fc-list | grep -i \"nanum\\|noto\"\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install openjdk-11-jdk -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406d894-c17c-4cab-b4a5-04286c4ad809",
   "metadata": {},
   "source": [
    "# Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19f7bfe-cb57-49ee-a4ea-bfcbbd35634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiyeo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '../data/'\n",
    "train = pd.read_csv(url+'train.csv')\n",
    "\n",
    "# 지연님 생성 데이터\n",
    "a = pd.read_csv(url+'general_dialog1.csv').rename(columns={'dialogue': 'conversation'})\n",
    "a['class'] = '일반 대화'\n",
    "a['idx'] = range(0,len(a))\n",
    "a = a[['idx','class','conversation']]\n",
    "\n",
    "# 유찬님 생성 데이터\n",
    "b = pd.read_csv(url+'general_dialog2.csv')\n",
    "b = b[b['class'] == \"일반 대화\"] # class에 일반대화가 아닌 conversation이 적혀있어 제거\n",
    "\n",
    "train = pd.concat([train, a, b], axis=0,ignore_index=True).drop(columns='idx')\n",
    "train.to_csv(url+\"train_w_general_conv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b09628e9-9a4e-48b9-b9d7-0e567fb92684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      class                                       conversation\n",
       "0           0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1           1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2           2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3           3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4           4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('./data/train_w_general_conv.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca7d5778-06d5-4880-9471-13e4e928da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f188590d-8777-4d47-a7fa-c2b1a105580d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                       conversation\n",
       "0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "006a3800-6a70-4a3e-b77a-227cd7b48c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>갈취 대화</th>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>기타 괴롭힘 대화</th>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>일반 대화</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>직장 내 괴롭힘 대화</th>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>협박 대화</th>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             conversation\n",
       "class                    \n",
       "갈취 대화                 981\n",
       "기타 괴롭힘 대화            1094\n",
       "일반 대화                1000\n",
       "직장 내 괴롭힘 대화           979\n",
       "협박 대화                 896"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90c16324-1aa2-4456-a65e-a3150f7e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "stop_words = {\n",
    "    '이', '그', '저', '것', '수', '등', '때', '곳', '나', '너', '우리', '경우', \n",
    "    '사람', '일', '지금', '생각', '말', '안', '뭐', '정말', '왜', '오늘', '내일',\n",
    "    '여기', '거기', '이제', '먼저', '하나', '무슨', '위해', '때문', '정도', \n",
    "    '그냥', '진짜', '너무', '완전', '혹시', '계속', '아니', '알', '더', '좀', '이다'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b2876c4-5349-40aa-97c4-ac3ba88520d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, stop_words):\n",
    "    # 1. 양쪽 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 2. 특수문자 및 이모지 제거 (한글, 영어, 숫자, 기본 구두점만 허용)\n",
    "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z.,!?~\\s]\", \" \", sentence)\n",
    "\n",
    "    # 3. 연속된 공백 하나로 축소 및 줄 바꿈 무시\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\n\", \" \", sentence)\n",
    "\n",
    "    # 4. 문장 부호 앞뒤로 공백 추가 (토큰 구분을 위함)\n",
    "    sentence = re.sub(r\"([?.!,~])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "    \n",
    "    # 형태소 분석 (단어, 품사)\n",
    "    include_tags = {\"Noun\", \"Verb\", \"Adjective\", \"Exclamation\", \"Adverb\"}\n",
    "    pos_tags = okt.pos(sentence, stem=True, norm=True)\n",
    "    # 원하는 품사만 추출\n",
    "    tokens = [\n",
    "        word for word, tag in pos_tags\n",
    "        if tag in include_tags and len(word) > 1 and word not in stop_words\n",
    "    ]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1337f999-c4c4-48ea-b49e-3a7249e9ec21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['지금', '스스로', '죽이다', '달라', '애원', '아니다', '죄송하다', '혼자', '죽지', '우리', '사건', '말리', '진짜', '죽이다', '버리다', '싶다', '정말', '선택', '죽다', '가족', '죽여주다', '죄송하다', '정말', '선택', '없다', '선택', '가족', '모조리', '죽이다', '버리다', '선택', '한번', '도와주다', '그냥', '죽이다', '버리다', '이의', '없다', '제발', '도와주다']\n"
     ]
    }
   ],
   "source": [
    "sample_text = raw_data['conversation'][0]\n",
    "tokens = preprocess_sentence(sample_text, stop_words)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a999961-c023-4792-ba7f-4fe6273f655b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "      <td>[지금, 스스로, 죽이다, 달라, 애원, 아니다, 죄송하다, 혼자, 죽지, 우리, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "      <td>[길동, 경찰서, 이다, 마트, 폭발물, 설치, 똑바로, 들다, 한번, 얘기, 장난...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "      <td>[되게, 귀엽다, 작다, 남자, 보다, 그만하다, 놀리다, 재미없다, 지영, 이지,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "      <td>[어이, 거기, 이리, 오라, 무슨, 좋다, 보이다, 있다, 보다, 아니다, 없다,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "      <td>[저기, 혹시, 너무, 뜨겁다, 저희, 회사, 선크림, 팔다, 손등, 발라, 보다,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                       conversation  \\\n",
       "0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...   \n",
       "1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...   \n",
       "2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...   \n",
       "3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...   \n",
       "4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [지금, 스스로, 죽이다, 달라, 애원, 아니다, 죄송하다, 혼자, 죽지, 우리, ...  \n",
       "1  [길동, 경찰서, 이다, 마트, 폭발물, 설치, 똑바로, 들다, 한번, 얘기, 장난...  \n",
       "2  [되게, 귀엽다, 작다, 남자, 보다, 그만하다, 놀리다, 재미없다, 지영, 이지,...  \n",
       "3  [어이, 거기, 이리, 오라, 무슨, 좋다, 보이다, 있다, 보다, 아니다, 없다,...  \n",
       "4  [저기, 혹시, 너무, 뜨겁다, 저희, 회사, 선크림, 팔다, 손등, 발라, 보다,...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['tokens'] = raw_data['conversation'].apply(lambda x: preprocess_sentence(str(x), stop_words))\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e4a25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "      <th>length</th>\n",
       "      <th>text_len</th>\n",
       "      <th>Adverb</th>\n",
       "      <th>Adverb_sw</th>\n",
       "      <th>Adjective</th>\n",
       "      <th>Adjective_sw</th>\n",
       "      <th>Exclamation</th>\n",
       "      <th>Exclamation_sw</th>\n",
       "      <th>Noun</th>\n",
       "      <th>Noun_sw</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Verb_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\r\\n 아닙니다. 죄송합니다.\\r\\n...</td>\n",
       "      <td>251</td>\n",
       "      <td>52</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['아니다', '죄송하다', '죄송하다', '없다', '없다']</td>\n",
       "      <td>['아니다', '죄송하다', '죄송하다', '없다', '없다']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['지금', '스스로', '달라', '애원', '혼자', '죽지', '우리', '사...</td>\n",
       "      <td>['스스로', '달라', '애원', '혼자', '죽지', '사건', '말리', '선...</td>\n",
       "      <td>['죽이다', '하다', '죽이다', '버리다', '싶다', '하다', '하다', ...</td>\n",
       "      <td>['죽이다', '하다', '죽이다', '버리다', '싶다', '하다', '하다', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\r\\n9시 40분 마트에 폭발물을 설치할거다.\\r\\n네?\\r\\n똑...</td>\n",
       "      <td>197</td>\n",
       "      <td>39</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['이다', '재미있다', '진정하다']</td>\n",
       "      <td>['재미있다', '진정하다']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['길동', '경찰서', '마트', '폭발물', '설치', '똑바로', '한번', ...</td>\n",
       "      <td>['길동', '경찰서', '마트', '폭발물', '설치', '똑바로', '한번', ...</td>\n",
       "      <td>['하다', '들다', '하다', '걸다', '말다', '터지다', '죽다', '되...</td>\n",
       "      <td>['하다', '들다', '하다', '걸다', '말다', '터지다', '죽다', '되...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\r\\n그만해. 니들 놀리는거 ...</td>\n",
       "      <td>227</td>\n",
       "      <td>52</td>\n",
       "      <td>['되게', '그만']</td>\n",
       "      <td>['되게', '그만']</td>\n",
       "      <td>['귀엽다', '작다', '그만하다', '재미없다', '그렇다', '좋다', '작다...</td>\n",
       "      <td>['귀엽다', '작다', '그만하다', '재미없다', '그렇다', '좋다', '작다...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['남자', '지영', '이지', '재는', '군대', '보태', '난쟁이', '장...</td>\n",
       "      <td>['남자', '지영', '이지', '재는', '군대', '보태', '난쟁이', '장...</td>\n",
       "      <td>['보다', '놀리다', '돼다', '가다', '주다', '가다', '가다', '보...</td>\n",
       "      <td>['보다', '놀리다', '돼다', '가다', '주다', '가다', '가다', '보...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\r\\n예??\\r\\n너 말이야 너. 이리 오라고\\r\\n무슨 일.\\r\\n너 ...</td>\n",
       "      <td>125</td>\n",
       "      <td>34</td>\n",
       "      <td>['이리']</td>\n",
       "      <td>['이리']</td>\n",
       "      <td>['좋다', '있다', '아니다', '없다', '있다', '없다']</td>\n",
       "      <td>['좋다', '있다', '아니다', '없다', '있다', '없다']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['어이', '거기', '오라', '무슨', '오늘', '피시방', '마지막', '...</td>\n",
       "      <td>['어이', '오라', '피시방', '마지막', '기회']</td>\n",
       "      <td>['보이다', '보다', '뒤지다', '나오다', '죽다', '내놓다']</td>\n",
       "      <td>['보이다', '보다', '뒤지다', '나오다', '죽다', '내놓다']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "      <td>471</td>\n",
       "      <td>118</td>\n",
       "      <td>['너무', '그래도', '따끔', '열심히', '많이', '많이', '많이', '...</td>\n",
       "      <td>['그래도', '따끔', '열심히', '많이', '많이', '많이', '빨리', '...</td>\n",
       "      <td>['뜨겁다', '필요하다', '좋다', '좋다', '좋다', '좋다', '같다', ...</td>\n",
       "      <td>['뜨겁다', '필요하다', '좋다', '좋다', '좋다', '좋다', '같다', ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['저기', '혹시', '저희', '회사', '선크림', '손등', '발라', '진...</td>\n",
       "      <td>['저기', '저희', '회사', '선크림', '손등', '발라', '선크림', '...</td>\n",
       "      <td>['팔다', '하다', '보다', '알아보다', '하다', '보다', '하다', '...</td>\n",
       "      <td>['팔다', '하다', '보다', '알아보다', '하다', '보다', '하다', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      class                                       conversation  \\\n",
       "0           0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\r\\n 아닙니다. 죄송합니다.\\r\\n...   \n",
       "1           1      협박 대화  길동경찰서입니다.\\r\\n9시 40분 마트에 폭발물을 설치할거다.\\r\\n네?\\r\\n똑...   \n",
       "2           2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\r\\n그만해. 니들 놀리는거 ...   \n",
       "3           3      갈취 대화  어이 거기\\r\\n예??\\r\\n너 말이야 너. 이리 오라고\\r\\n무슨 일.\\r\\n너 ...   \n",
       "4           4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...   \n",
       "\n",
       "   length  text_len                                             Adverb  \\\n",
       "0     251        52                                                 []   \n",
       "1     197        39                                                 []   \n",
       "2     227        52                                       ['되게', '그만']   \n",
       "3     125        34                                             ['이리']   \n",
       "4     471       118  ['너무', '그래도', '따끔', '열심히', '많이', '많이', '많이', '...   \n",
       "\n",
       "                                           Adverb_sw  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                       ['되게', '그만']   \n",
       "3                                             ['이리']   \n",
       "4  ['그래도', '따끔', '열심히', '많이', '많이', '많이', '빨리', '...   \n",
       "\n",
       "                                           Adjective  \\\n",
       "0                ['아니다', '죄송하다', '죄송하다', '없다', '없다']   \n",
       "1                             ['이다', '재미있다', '진정하다']   \n",
       "2  ['귀엽다', '작다', '그만하다', '재미없다', '그렇다', '좋다', '작다...   \n",
       "3              ['좋다', '있다', '아니다', '없다', '있다', '없다']   \n",
       "4  ['뜨겁다', '필요하다', '좋다', '좋다', '좋다', '좋다', '같다', ...   \n",
       "\n",
       "                                        Adjective_sw Exclamation  \\\n",
       "0                ['아니다', '죄송하다', '죄송하다', '없다', '없다']          []   \n",
       "1                                   ['재미있다', '진정하다']          []   \n",
       "2  ['귀엽다', '작다', '그만하다', '재미없다', '그렇다', '좋다', '작다...          []   \n",
       "3              ['좋다', '있다', '아니다', '없다', '있다', '없다']          []   \n",
       "4  ['뜨겁다', '필요하다', '좋다', '좋다', '좋다', '좋다', '같다', ...          []   \n",
       "\n",
       "  Exclamation_sw                                               Noun  \\\n",
       "0             []  ['지금', '스스로', '달라', '애원', '혼자', '죽지', '우리', '사...   \n",
       "1             []  ['길동', '경찰서', '마트', '폭발물', '설치', '똑바로', '한번', ...   \n",
       "2             []  ['남자', '지영', '이지', '재는', '군대', '보태', '난쟁이', '장...   \n",
       "3             []  ['어이', '거기', '오라', '무슨', '오늘', '피시방', '마지막', '...   \n",
       "4             []  ['저기', '혹시', '저희', '회사', '선크림', '손등', '발라', '진...   \n",
       "\n",
       "                                             Noun_sw  \\\n",
       "0  ['스스로', '달라', '애원', '혼자', '죽지', '사건', '말리', '선...   \n",
       "1  ['길동', '경찰서', '마트', '폭발물', '설치', '똑바로', '한번', ...   \n",
       "2  ['남자', '지영', '이지', '재는', '군대', '보태', '난쟁이', '장...   \n",
       "3                   ['어이', '오라', '피시방', '마지막', '기회']   \n",
       "4  ['저기', '저희', '회사', '선크림', '손등', '발라', '선크림', '...   \n",
       "\n",
       "                                                Verb  \\\n",
       "0  ['죽이다', '하다', '죽이다', '버리다', '싶다', '하다', '하다', ...   \n",
       "1  ['하다', '들다', '하다', '걸다', '말다', '터지다', '죽다', '되...   \n",
       "2  ['보다', '놀리다', '돼다', '가다', '주다', '가다', '가다', '보...   \n",
       "3           ['보이다', '보다', '뒤지다', '나오다', '죽다', '내놓다']   \n",
       "4  ['팔다', '하다', '보다', '알아보다', '하다', '보다', '하다', '...   \n",
       "\n",
       "                                             Verb_sw  \n",
       "0  ['죽이다', '하다', '죽이다', '버리다', '싶다', '하다', '하다', ...  \n",
       "1  ['하다', '들다', '하다', '걸다', '말다', '터지다', '죽다', '되...  \n",
       "2  ['보다', '놀리다', '돼다', '가다', '주다', '가다', '가다', '보...  \n",
       "3           ['보이다', '보다', '뒤지다', '나오다', '죽다', '내놓다']  \n",
       "4  ['팔다', '하다', '보다', '알아보다', '하다', '보다', '하다', '...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens = pd.read_csv('../data/train_w_pos_list.csv')\n",
    "train_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "edecb265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9531 | Labels: {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
      "torch.Size([32, 61])\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Vocab 빌드 =====\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "import json\n",
    "\n",
    "SPECIALS = [\"<pad>\", \"<unk>\"]\n",
    "\n",
    "def build_vocab(\n",
    "    token_lists: Iterable[List[str]],\n",
    "    min_freq: int = 2,\n",
    "    max_size: int = 30000,\n",
    "    specials: List[str] = SPECIALS,\n",
    ") -> Tuple[Dict[str, int], List[str], Counter]:\n",
    "    \"\"\"\n",
    "    token_lists: 각 샘플의 토큰 리스트(iterable of list[str])\n",
    "    min_freq: 최소 등장 빈도 미만 토큰은 제외\n",
    "    max_size: special 포함 전체 vocab 상한 (None이면 제한 없음)\n",
    "    returns: (stoi, itos, counter)\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for toks in token_lists:\n",
    "        counter.update(toks)\n",
    "\n",
    "    # 빈도 필터 + 상위 max_size-특수토큰 만큼\n",
    "    most = [tok for tok, cnt in counter.most_common() if cnt >= min_freq]\n",
    "    if max_size is not None:\n",
    "        cap = max_size - len(specials)\n",
    "        most = most[:max(0, cap)]\n",
    "\n",
    "    itos = list(specials) + most\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos, counter\n",
    "\n",
    "def save_vocab(path: str, itos: List[str]) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(itos, f, ensure_ascii=False)\n",
    "\n",
    "def load_vocab(path: str) -> Tuple[Dict[str, int], List[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        itos = json.load(f)\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "# ===== 2) 토큰 → ID 인코딩 =====\n",
    "def encode_tokens(\n",
    "    tokens: List[str],\n",
    "    stoi: Dict[str, int],\n",
    "    max_len: int = 256,\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    tokens -> input_ids, attention_mask\n",
    "    - OOV는 <unk>\n",
    "    - max_len을 초과하면 적절히 자름\n",
    "    \"\"\"\n",
    "    pad_id = stoi[\"<pad>\"]\n",
    "    unk_id = stoi[\"<unk>\"]\n",
    "\n",
    "    ids = [stoi.get(t, unk_id) for t in tokens]\n",
    "\n",
    "    # 길이 계산 (cls/sep 포함해서 자르기)\n",
    "    keep = max_len\n",
    "    keep = max(0, keep)\n",
    "    ids = ids[:keep]\n",
    "\n",
    "    attn = [1] * len(ids)\n",
    "    return ids, attn\n",
    "# ===== 3) 배치 패딩(collate) =====\n",
    "import torch\n",
    "\n",
    "def collate_batch(\n",
    "    batch,\n",
    "    pad_id: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    batch: [{\"input_ids\": List[int], \"attention_mask\": List[int], \"label\": int}, ...]\n",
    "    \"\"\"\n",
    "    bs = len(batch)\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids = torch.full((bs, maxlen), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((bs, maxlen), dtype=torch.long)\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "\n",
    "    for i, x in enumerate(batch):\n",
    "        L = len(x[\"input_ids\"])\n",
    "        input_ids[i, :L] = torch.tensor(x[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask[i, :L] = torch.tensor(x[\"attention_mask\"], dtype=torch.long)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "# ===== 4) 예시 파이프라인 (라벨 매핑 포함) =====\n",
    "# 4-1) tokens 컬럼이 없다면 먼저 생성\n",
    "\n",
    "import ast\n",
    "stopwords = {'하다', '보다', '알다', '가다', '되다', '돼다', '오다', '진짜', '지금', '사람', '우리', '오늘', '생각', '그냥', '무슨'}\n",
    "cols = [\"Noun\", \"Verb\", \"Adjective\", \"Adverb\", \"Exclamation\"]\n",
    "epochs = 20\n",
    "\n",
    "all_stopwords = stopwords | stop_words\n",
    "\n",
    "train_tokens[\"tokens\"] = train_tokens[cols].apply(\n",
    "    lambda row: sum((r for r in row if isinstance(r, list)), []),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "def clean_token(t: str) -> str:\n",
    "    if not isinstance(t, str):\n",
    "        t = str(t)\n",
    "    t = t.strip()\n",
    "    # 1) 앞/뒤에 붙은 따옴표 제거\n",
    "    t = re.sub(r\"^[\\\"']+\", \"\", t)   # 맨 앞의 ' 또는 \" 연속 제거\n",
    "    t = re.sub(r\"[\\\"']+$\", \"\", t)   # 맨 뒤의 ' 또는 \" 연속 제거\n",
    "    # 2) 남아있을 수 있는 괄호/대괄호/쉼표 잔여 제거\n",
    "    t = t.strip(\"[],\")\n",
    "    return t.strip()\n",
    "\n",
    "# tokens 컬럼 클린업 (빈 문자열은 제거)\n",
    "train_tokens[\"tokens\"] = train_tokens[\"tokens\"].apply(\n",
    "    lambda toks: [clean_token(x) for x in toks if clean_token(x)]\n",
    ")\n",
    "\n",
    "# 불용어 제거 (통합 집합 사용)\n",
    "all_stopwords = stopwords | stop_words\n",
    "train_tokens[\"tokens_sw\"] = train_tokens[\"tokens\"].apply(\n",
    "    lambda toks: [t for t in toks if t and t not in all_stopwords]\n",
    ")\n",
    "\n",
    "# 4-2) 라벨 매핑\n",
    "labels = sorted(train_tokens[\"class\"].unique().tolist())\n",
    "label2id = {\n",
    "    \"협박 대화\": 0,\n",
    "    \"갈취 대화\": 1,\n",
    "    \"직장 내 괴롭힘 대화\": 2,\n",
    "    \"기타 괴롭힘 대화\": 3,\n",
    "    \"일반 대화\": 4,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# 4-3) vocab 빌드\n",
    "stoi, itos, counter = build_vocab(train_tokens[\"tokens_sw\"], min_freq=1, max_size=20000)\n",
    "pad_id = stoi[\"<pad>\"]\n",
    "\n",
    "# 4-4) 인코딩 (train/valid 분할은 이미 되어있다고 가정하거나 아래처럼 간단 분할)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(train_tokens, test_size=0.2, random_state=42, stratify=train_tokens[\"class\"])\n",
    "\n",
    "def encode_row(row, max_len=256):\n",
    "    ids, attn = encode_tokens(row[\"tokens_sw\"], stoi, max_len=max_len)\n",
    "    return {\n",
    "        \"input_ids\": ids,\n",
    "        \"attention_mask\": attn,\n",
    "        \"label\": label2id[row[\"class\"]],\n",
    "    }\n",
    "\n",
    "train_records = [encode_row(r) for _, r in train_df.iterrows()]\n",
    "valid_records = [encode_row(r) for _, r in valid_df.iterrows()]\n",
    "\n",
    "# 4-5) PyTorch Dataset/Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleListDataset(Dataset):\n",
    "    def __init__(self, records):\n",
    "        self.records = records\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.records[idx]\n",
    "\n",
    "train_ds = SimpleListDataset(train_records)\n",
    "valid_ds = SimpleListDataset(valid_records)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,\n",
    "                          collate_fn=lambda b: collate_batch(b, pad_id))\n",
    "valid_loader = DataLoader(valid_ds, batch_size=32, shuffle=False,\n",
    "                          collate_fn=lambda b: collate_batch(b, pad_id))\n",
    "\n",
    "print(f\"Vocab size: {len(itos)} | Labels: {label2id}\")\n",
    "print(next(iter(train_loader))[\"input_ids\"].shape)  # (B, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "836b9c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['지금',\n",
       " '스스로',\n",
       " '달라',\n",
       " '애원',\n",
       " '혼자',\n",
       " '죽지',\n",
       " '우리',\n",
       " '사건',\n",
       " '말리',\n",
       " '진짜',\n",
       " '정말',\n",
       " '선택',\n",
       " '가족',\n",
       " '정말',\n",
       " '선택',\n",
       " '선택',\n",
       " '가족',\n",
       " '모조리',\n",
       " '선택',\n",
       " '한번',\n",
       " '그냥',\n",
       " '이의',\n",
       " '제발',\n",
       " '죽이다',\n",
       " '하다',\n",
       " '죽이다',\n",
       " '버리다',\n",
       " '싶다',\n",
       " '하다',\n",
       " '하다',\n",
       " '죽다',\n",
       " '죽여주다',\n",
       " '하다',\n",
       " '하다',\n",
       " '죽이다',\n",
       " '버리다',\n",
       " '하다',\n",
       " '도와주다',\n",
       " '죽이다',\n",
       " '버리다',\n",
       " '도와주다',\n",
       " '아니다',\n",
       " '죄송하다',\n",
       " '죄송하다',\n",
       " '없다',\n",
       " '없다']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d36f6fea-ab46-4751-8b7f-1f07ee3f72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# ----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len,1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "        # 미세한 안정화용\n",
    "        nn.init.zeros_(self.pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, S, E)\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5da2d1df-9e6e-4c1e-8152-cb0ee761d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Transformer Encoder Classifier\n",
    "# ----------------------------\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    순수 Transformer-Encoder 기반 문서/대화 분류기.\n",
    "    - input_ids: (B, S) 토큰 인덱스\n",
    "    - attention_mask: (B, S) 1=유효, 0=패딩\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_classes: int,\n",
    "        emb_dim: int = 256,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dim_feedforward: int = 512,\n",
    "        max_len: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        pad_id: int = 0,\n",
    "        use_cls_pool: bool = True,  # True면 첫 토큰(<cls>)을 문장 표현으로 사용, False면 마스크 평균\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert emb_dim % nhead == 0, \"emb_dim must be divisible by nhead\"\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.pos = PositionalEncoding(emb_dim, max_len=max_len, dropout=dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=False,  # 입력은 (S,B,E)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "        self.emb_scale = math.sqrt(emb_dim)\n",
    "        self.use_cls_pool = use_cls_pool\n",
    "\n",
    "        # Xavier init (선택)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_repr: bool = False,\n",
    "    ):\n",
    "        # (B,S) -> (B,S,E)\n",
    "        x = self.emb(input_ids) * self.emb_scale\n",
    "        x = self.pos(x)                         # (B,S,E)\n",
    "        x = x.transpose(0, 1)                   # (S,B,E)\n",
    "\n",
    "        key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = (attention_mask == 0)  # True=mask\n",
    "\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)  # (S,B,E)\n",
    "        x = x.transpose(0, 1)                                       # (B,S,E)\n",
    "\n",
    "        if self.use_cls_pool:\n",
    "            sent_repr = x[:, 0, :]  # <cls> 위치\n",
    "        else:\n",
    "            if attention_mask is None:\n",
    "                sent_repr = x.mean(dim=1)\n",
    "            else:\n",
    "                mask = attention_mask.unsqueeze(-1).float()         # (B,S,1)\n",
    "                sent_repr = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
    "\n",
    "        sent_repr = self.norm(sent_repr)\n",
    "        logits = self.classifier(sent_repr)\n",
    "\n",
    "        if return_repr:\n",
    "            return logits, sent_repr\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7063c91e-b16d-4371-b196-755941f48d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 학습/평가 루프\n",
    "# ----------------------------\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    class_weights: Optional[torch.Tensor] = None,\n",
    "    grad_clip: float = 1.0,\n",
    "    scheduler = None,\n",
    "    use_amp: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    ce = nn.CrossEntropyLoss(weight=class_weights.to(device) if class_weights is not None else None)\n",
    "\n",
    "    losses, all_preds, all_labels = [], [], []\n",
    "    for batch in tqdm(dataloader, desc=\"train\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch.get(\"attention_mask\")\n",
    "        attn = attn.to(device) if attn is not None else None\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(input_ids, attention_mask=attn)\n",
    "            loss = ce(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if grad_clip is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        all_preds += logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "        all_labels += labels.detach().cpu().tolist()\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return {\"loss\": sum(losses)/len(losses), \"acc\": acc, \"f1_macro\": f1}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses, all_preds, all_labels = [], [], []\n",
    "    for batch in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch.get(\"attention_mask\")\n",
    "        attn = attn.to(device) if attn is not None else None\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask=attn)\n",
    "        loss = ce(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        all_preds += logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "        all_labels += labels.detach().cpu().tolist()\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\") if all_labels else 0.0\n",
    "    return {\"loss\": sum(losses)/len(losses), \"acc\": acc, \"f1_macro\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d90593ed-d7e6-444c-bc92-9c40cadd0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 모델 팩토리 (간단 생성기)\n",
    "# ----------------------------\n",
    "def create_model(\n",
    "    vocab_size: int,\n",
    "    num_classes: int = 5,          # 협박0, 갈취1, 직장2, 기타3, 일반4\n",
    "    pad_id: int = 0,\n",
    "    emb_dim: int = 256,\n",
    "    nhead: int = 8,\n",
    "    num_layers: int = 4,\n",
    "    dim_ff: int = 512,\n",
    "    max_len: int = 512,\n",
    "    dropout: float = 0.1,\n",
    "    use_cls_pool: bool = True,\n",
    ") -> nn.Module:\n",
    "    return TransformerClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        emb_dim=emb_dim,\n",
    "        nhead=nhead,\n",
    "        num_layers=num_layers,\n",
    "        dim_feedforward=dim_ff,\n",
    "        max_len=max_len,\n",
    "        dropout=dropout,\n",
    "        pad_id=pad_id,\n",
    "        use_cls_pool=use_cls_pool,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "10671bf9-831f-41a5-84f3-ac4b460d37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가정: stoi, itos, train_loader, valid_loader, label2id 존재\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = create_model(\n",
    "    vocab_size=len(itos),\n",
    "    num_classes=5,                # 고정 매핑(협박0, 갈취1, 직장2, 기타3, 일반4)\n",
    "    pad_id=stoi[\"<pad>\"],\n",
    "    emb_dim=256,\n",
    "    nhead=8,\n",
    "    num_layers=3,                 # 처음엔 3~4로 시작 추천\n",
    "    dim_ff=512,\n",
    "    max_len=256,                  # 인코딩에서 쓴 max_len과 동일하게\n",
    "    dropout=0.1,\n",
    "    use_cls_pool=True,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "scheduler = None  # 필요하면 CosineAnnealingLR 등 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3da6f069-1579-4b13-8299-834c9ba5660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train: {'loss': 1.1870362875968452, 'acc': 0.5460624071322436, 'f1_macro': 0.5415067864113065} | valid: {'loss': 0.8939134161919355, 'acc': 0.6693069306930693, 'f1_macro': 0.6609375017011339}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02] train: {'loss': 0.652233277249524, 'acc': 0.7644873699851411, 'f1_macro': 0.7612607992640013} | valid: {'loss': 0.707449602894485, 'acc': 0.7435643564356436, 'f1_macro': 0.7386206372168495}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03] train: {'loss': 0.4752842848460505, 'acc': 0.8315998018821199, 'f1_macro': 0.8291924363214177} | valid: {'loss': 0.7025370709598064, 'acc': 0.7584158415841584, 'f1_macro': 0.755839067427152}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04] train: {'loss': 0.35462314881912366, 'acc': 0.866765725606736, 'f1_macro': 0.8647911644998638} | valid: {'loss': 0.7096115606836975, 'acc': 0.7643564356435644, 'f1_macro': 0.7593688159916184}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05] train: {'loss': 0.282549075899631, 'acc': 0.8984645864289252, 'f1_macro': 0.8967606198915019} | valid: {'loss': 0.7374368305318058, 'acc': 0.7792079207920792, 'f1_macro': 0.7762914396281936}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06] train: {'loss': 0.20597267690606005, 'acc': 0.9309063893016345, 'f1_macro': 0.9297230496370457} | valid: {'loss': 0.783844695892185, 'acc': 0.7801980198019802, 'f1_macro': 0.7758827499126026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07] train: {'loss': 0.17060229449173597, 'acc': 0.9380881624566617, 'f1_macro': 0.9369508438132602} | valid: {'loss': 0.8267900026403368, 'acc': 0.7881188118811882, 'f1_macro': 0.7835696383102848}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08] train: {'loss': 0.13538128305886557, 'acc': 0.9517087667161961, 'f1_macro': 0.9509930784327792} | valid: {'loss': 0.856289628893137, 'acc': 0.7910891089108911, 'f1_macro': 0.7876510687939001}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09] train: {'loss': 0.118821156244197, 'acc': 0.95789995047053, 'f1_macro': 0.957244682260388} | valid: {'loss': 0.9495013216510415, 'acc': 0.7772277227722773, 'f1_macro': 0.7734740424479443}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] train: {'loss': 0.09327781329657442, 'acc': 0.966319960376424, 'f1_macro': 0.9659255326453436} | valid: {'loss': 1.008877214975655, 'acc': 0.7861386138613862, 'f1_macro': 0.7797125863377754}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] train: {'loss': 0.09246088092134694, 'acc': 0.9653293709757306, 'f1_macro': 0.9648410869163975} | valid: {'loss': 1.0634757606312633, 'acc': 0.7960396039603961, 'f1_macro': 0.7918684795148287}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12] train: {'loss': 0.07469012699028987, 'acc': 0.9732540861812778, 'f1_macro': 0.9726568123309886} | valid: {'loss': 1.170522352680564, 'acc': 0.7930693069306931, 'f1_macro': 0.7874220247211134}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] train: {'loss': 0.0757397032828908, 'acc': 0.9769687964338781, 'f1_macro': 0.9765570712461044} | valid: {'loss': 1.1868348885327578, 'acc': 0.7910891089108911, 'f1_macro': 0.7876722994237964}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14] train: {'loss': 0.04848632534441546, 'acc': 0.9831599801882119, 'f1_macro': 0.9829581101171033} | valid: {'loss': 1.294426467269659, 'acc': 0.7891089108910891, 'f1_macro': 0.7852370070362993}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] train: {'loss': 0.05824377340794904, 'acc': 0.9816740960871718, 'f1_macro': 0.9814271862446666} | valid: {'loss': 1.4290023855865002, 'acc': 0.8, 'f1_macro': 0.7973447056609324}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] train: {'loss': 0.04591781822095731, 'acc': 0.9831599801882119, 'f1_macro': 0.9831006666971416} | valid: {'loss': 1.5077247638255358, 'acc': 0.7910891089108911, 'f1_macro': 0.78711101443547}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] train: {'loss': 0.05298539988527452, 'acc': 0.9826646854878652, 'f1_macro': 0.9825345022888141} | valid: {'loss': 1.5635885084047914, 'acc': 0.806930693069307, 'f1_macro': 0.804322278913521}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] train: {'loss': 0.03580034854731341, 'acc': 0.9898464586428926, 'f1_macro': 0.9897002816533724} | valid: {'loss': 1.5799787510186434, 'acc': 0.800990099009901, 'f1_macro': 0.7990896186178064}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] train: {'loss': 0.03787967900994615, 'acc': 0.9903417533432393, 'f1_macro': 0.9902123034120741} | valid: {'loss': 1.4758273772895336, 'acc': 0.804950495049505, 'f1_macro': 0.8014603777257909}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] train: {'loss': 0.033502606170145555, 'acc': 0.9900941059930659, 'f1_macro': 0.9899919329989008} | valid: {'loss': 1.5958001390099525, 'acc': 0.799009900990099, 'f1_macro': 0.7960687166284792}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# (선택) 클래스 가중치: train_df의 정수 라벨 리스트로 계산\n",
    "from collections import Counter\n",
    "train_labels = [rec[\"label\"] for rec in train_records]  # 이전 단계 encode_records 기준\n",
    "cnt = Counter(train_labels)\n",
    "weights = torch.tensor([1.0 / max(cnt.get(i, 1), 1) for i in range(5)], dtype=torch.float)\n",
    "weights = weights / weights.mean()  # 평균 1로 정규화\n",
    "class_weights = weights\n",
    "\n",
    "best_f1 = 0.0\n",
    "for ep in range(1, epochs+1):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, device,\n",
    "                         class_weights=class_weights, grad_clip=1.0, scheduler=scheduler, use_amp=True)\n",
    "    va = evaluate(model, valid_loader, device)\n",
    "    print(f\"[{ep:02d}] train: {tr} | valid: {va}\")\n",
    "\n",
    "    if va[\"f1_macro\"] > best_f1:\n",
    "        best_f1 = va[\"f1_macro\"]\n",
    "        torch.save(model.state_dict(), \"./stopwords1_\"+str(best_f1)+\"_best_transformer_cls.pt\")\n",
    "        print(\"  ✔ saved best model (F1 ↑)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06dd412",
   "metadata": {},
   "source": [
    "## Stopword로 일반 대화 토큰 전부 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9f54b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9403 | Labels: {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
      "torch.Size([32, 94])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "general_rows = train_tokens[train_tokens['class'] == '일반 대화']\n",
    "\n",
    "def collect_tokens_by_cols(df, cols):\n",
    "    \"\"\"지정한 품사 컬럼들에서 토큰을 모아 clean_token 적용 후 반환\"\"\"\n",
    "    bag = []\n",
    "    for tag in cols:\n",
    "        if tag in df.columns:\n",
    "            for lst in df[tag].dropna():\n",
    "                if isinstance(lst, (list, tuple)):\n",
    "                    bag.extend(clean_token(t) for t in lst if clean_token(t))\n",
    "    return [t for t in bag if t]  # 빈 문자열 제거\n",
    "\n",
    "# 1) 일반 대화 + 지정 품사에서 토큰 수집\n",
    "general_tokens = collect_tokens_by_cols(general_rows, cols)\n",
    "\n",
    "# 2) 이미 등록된 불용어는 제외하고 빈도 계산\n",
    "base_stop = stopwords | stop_words\n",
    "freq = Counter(t for t in general_tokens if t not in base_stop)\n",
    "\n",
    "# 3) 누적 80%를 커버하는 최소 토큰 집합 선택\n",
    "coverage = 0.80\n",
    "total = sum(freq.values())\n",
    "\n",
    "top80_new_stops = set()\n",
    "if total > 0:\n",
    "    cum = 0\n",
    "    for tok, cnt in freq.most_common():  # 빈도 내림차순\n",
    "        cum += cnt\n",
    "        top80_new_stops.add(tok)\n",
    "        if cum / total >= coverage:\n",
    "            break\n",
    "\n",
    "# 4) 최종 불용어 집합 갱신\n",
    "all_stopwords2 = base_stop | top80_new_stops\n",
    "\n",
    "# 5) 적용 컬럼 생성/갱신 (tokens_sw2)\n",
    "train_tokens[\"tokens_sw2\"] = train_tokens[\"tokens\"].apply(\n",
    "    lambda toks: [t for t in toks if t and t not in all_stopwords2]\n",
    ")\n",
    "\n",
    "# 4-3) vocab 빌드\n",
    "stoi, itos, counter = build_vocab(train_tokens[\"tokens_sw2\"], min_freq=1, max_size=20000)\n",
    "pad_id = stoi[\"<pad>\"]\n",
    "\n",
    "# 4-4) 인코딩 (train/valid 분할은 이미 되어있다고 가정하거나 아래처럼 간단 분할)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(train_tokens, test_size=0.2, random_state=42, stratify=train_tokens[\"class\"])\n",
    "\n",
    "def encode_row(row, max_len=256):\n",
    "    ids, attn = encode_tokens(row[\"tokens_sw2\"], stoi, max_len=max_len)\n",
    "    return {\n",
    "        \"input_ids\": ids,\n",
    "        \"attention_mask\": attn,\n",
    "        \"label\": label2id[row[\"class\"]],\n",
    "    }\n",
    "\n",
    "train_records = [encode_row(r) for _, r in train_df.iterrows()]\n",
    "valid_records = [encode_row(r) for _, r in valid_df.iterrows()]\n",
    "\n",
    "# 4-5) PyTorch Dataset/Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleListDataset(Dataset):\n",
    "    def __init__(self, records):\n",
    "        self.records = records\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.records[idx]\n",
    "\n",
    "train_ds = SimpleListDataset(train_records)\n",
    "valid_ds = SimpleListDataset(valid_records)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,\n",
    "                          collate_fn=lambda b: collate_batch(b, pad_id))\n",
    "valid_loader = DataLoader(valid_ds, batch_size=32, shuffle=False,\n",
    "                          collate_fn=lambda b: collate_batch(b, pad_id))\n",
    "\n",
    "print(f\"Vocab size: {len(itos)} | Labels: {label2id}\")\n",
    "print(next(iter(train_loader))[\"input_ids\"].shape)  # (B, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "17c21529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가정: stoi, itos, train_loader, valid_loader, label2id 존재\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = create_model(\n",
    "    vocab_size=len(itos),\n",
    "    num_classes=5,                # 고정 매핑(협박0, 갈취1, 직장2, 기타3, 일반4)\n",
    "    pad_id=stoi[\"<pad>\"],\n",
    "    emb_dim=256,\n",
    "    nhead=8,\n",
    "    num_layers=3,                 # 처음엔 3~4로 시작 추천\n",
    "    dim_ff=512,\n",
    "    max_len=256,                  # 인코딩에서 쓴 max_len과 동일하게\n",
    "    dropout=0.1,\n",
    "    use_cls_pool=True,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "scheduler = None  # 필요하면 CosineAnnealingLR 등 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1a9bf78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train: {'loss': 1.247204883830754, 'acc': 0.5111441307578009, 'f1_macro': 0.5086680611928837} | valid: {'loss': 0.860168443992734, 'acc': 0.6693069306930693, 'f1_macro': 0.6672323583311806}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02] train: {'loss': 0.6956655461722472, 'acc': 0.7322932144626052, 'f1_macro': 0.7295971923484854} | valid: {'loss': 0.8299516383558512, 'acc': 0.7039603960396039, 'f1_macro': 0.6985639420512975}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03] train: {'loss': 0.5026819986386561, 'acc': 0.8199603764239722, 'f1_macro': 0.8171912316052976} | valid: {'loss': 0.6862445455044508, 'acc': 0.7653465346534654, 'f1_macro': 0.7634188255247354}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04] train: {'loss': 0.3541648086952412, 'acc': 0.8764239722634968, 'f1_macro': 0.8749730360192464} | valid: {'loss': 0.7327685505151749, 'acc': 0.7663366336633664, 'f1_macro': 0.7638124662164618}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05] train: {'loss': 0.2806653075917499, 'acc': 0.9011887072808321, 'f1_macro': 0.8995479788932664} | valid: {'loss': 0.7693733274936676, 'acc': 0.7841584158415842, 'f1_macro': 0.7819244790460199}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06] train: {'loss': 0.21185703525745023, 'acc': 0.9257057949479941, 'f1_macro': 0.9246715017318852} | valid: {'loss': 0.7490243958309293, 'acc': 0.7930693069306931, 'f1_macro': 0.7907693003131007}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07] train: {'loss': 0.16437088221487567, 'acc': 0.9437840515106488, 'f1_macro': 0.9429927418145324} | valid: {'loss': 0.8225733861327171, 'acc': 0.801980198019802, 'f1_macro': 0.7989661818373613}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08] train: {'loss': 0.1325972052951028, 'acc': 0.9514611193660227, 'f1_macro': 0.9507486360641979} | valid: {'loss': 0.8134371312335134, 'acc': 0.806930693069307, 'f1_macro': 0.8044624786715922}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09] train: {'loss': 0.10444111777121186, 'acc': 0.9613670133729569, 'f1_macro': 0.9609318114760115} | valid: {'loss': 0.954157424159348, 'acc': 0.802970297029703, 'f1_macro': 0.7998211534251947}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] train: {'loss': 0.09656824067685987, 'acc': 0.967062902426944, 'f1_macro': 0.9666766906304192} | valid: {'loss': 1.036713121458888, 'acc': 0.8, 'f1_macro': 0.7971369291750475}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] train: {'loss': 0.09084439429722169, 'acc': 0.9675581971272907, 'f1_macro': 0.9669013267272618} | valid: {'loss': 1.0838420754298568, 'acc': 0.800990099009901, 'f1_macro': 0.7996739531476675}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12] train: {'loss': 0.06922743726392482, 'acc': 0.9752352649826647, 'f1_macro': 0.9750057573252752} | valid: {'loss': 1.2253995956853032, 'acc': 0.7980198019801981, 'f1_macro': 0.7947734996385529}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] train: {'loss': 0.07438762203650916, 'acc': 0.9752352649826647, 'f1_macro': 0.9747429449186399} | valid: {'loss': 1.2021596059203148, 'acc': 0.800990099009901, 'f1_macro': 0.7980946092772578}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14] train: {'loss': 0.04965884260985428, 'acc': 0.9821693907875185, 'f1_macro': 0.9818848802744148} | valid: {'loss': 1.3203552020713687, 'acc': 0.8, 'f1_macro': 0.7955818871358387}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] train: {'loss': 0.047772274478191996, 'acc': 0.9821693907875185, 'f1_macro': 0.9819590101954241} | valid: {'loss': 1.4275393821299076, 'acc': 0.7980198019801981, 'f1_macro': 0.7953728231567873}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] train: {'loss': 0.06358970182861419, 'acc': 0.9811788013868251, 'f1_macro': 0.9808905262393974} | valid: {'loss': 1.2991219451650977, 'acc': 0.808910891089109, 'f1_macro': 0.8056129930400292}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] train: {'loss': 0.03609556381624008, 'acc': 0.986627043090639, 'f1_macro': 0.9865791726399669} | valid: {'loss': 1.3946415148675442, 'acc': 0.806930693069307, 'f1_macro': 0.8053627449501064}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] train: {'loss': 0.03335814001294833, 'acc': 0.9881129271916791, 'f1_macro': 0.9880938735831866} | valid: {'loss': 1.5355686247348785, 'acc': 0.7970297029702971, 'f1_macro': 0.7952278185123836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] train: {'loss': 0.04809915552750037, 'acc': 0.9861317483902923, 'f1_macro': 0.9861251621255974} | valid: {'loss': 1.5893879812210798, 'acc': 0.807920792079208, 'f1_macro': 0.8058296280644857}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] train: {'loss': 0.04659026393323805, 'acc': 0.9858841010401189, 'f1_macro': 0.9857786889047414} | valid: {'loss': 1.6903959326446056, 'acc': 0.808910891089109, 'f1_macro': 0.8087226104781735}\n",
      "  ✔ saved best model (F1 ↑)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# (선택) 클래스 가중치: train_df의 정수 라벨 리스트로 계산\n",
    "from collections import Counter\n",
    "train_labels = [rec[\"label\"] for rec in train_records]  # 이전 단계 encode_records 기준\n",
    "cnt = Counter(train_labels)\n",
    "weights = torch.tensor([1.0 / max(cnt.get(i, 1), 1) for i in range(5)], dtype=torch.float)\n",
    "weights = weights / weights.mean()  # 평균 1로 정규화\n",
    "class_weights = weights\n",
    "\n",
    "best_f1 = 0.0\n",
    "for ep in range(1, epochs+1):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, device,\n",
    "                         class_weights=class_weights, grad_clip=1.0, scheduler=scheduler, use_amp=True)\n",
    "    va = evaluate(model, valid_loader, device)\n",
    "    print(f\"[{ep:02d}] train: {tr} | valid: {va}\")\n",
    "\n",
    "    if va[\"f1_macro\"] > best_f1:\n",
    "        best_f1 = va[\"f1_macro\"]\n",
    "        torch.save(model.state_dict(), \"./stopwords2_\"+str(best_f1)+\"_best_transformer_cls.pt\")\n",
    "        print(\"  ✔ saved best model (F1 ↑)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30491f9d-28fa-45b0-b4d2-007457c0e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Dict, Callable, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ------------------------------\n",
    "# 내부용 Dataset / collate\n",
    "# ------------------------------\n",
    "class _ListDataset(Dataset):\n",
    "    def __init__(self, items): self.items = items\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return self.items[i]\n",
    "\n",
    "def _collate_batch(batch, pad_id):\n",
    "    bs = len(batch)\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids = torch.full((bs, maxlen), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((bs, maxlen), dtype=torch.long)\n",
    "    for i, x in enumerate(batch):\n",
    "        L = len(x[\"input_ids\"])\n",
    "        input_ids[i, :L] = torch.tensor(x[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask[i, :L] = torch.tensor(x[\"attention_mask\"], dtype=torch.long)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "# ------------------------------\n",
    "# 메인 함수\n",
    "# ------------------------------\n",
    "def fill_submission_class(\n",
    "    test_csv_path: str,\n",
    "    submission_csv_path: str,\n",
    "    model: torch.nn.Module,\n",
    "    stoi: Dict[str, int],\n",
    "    encode_tokens: Callable[[List[str], Dict[str, int], int, bool, bool], tuple],\n",
    "    preprocess_fn: Optional[Callable[[str, set], List[str]]] = None,\n",
    "    stop_words: Optional[set] = None,\n",
    "    id2label: Optional[Dict[int, str]] = None,\n",
    "    input_text_col: str = \"conversation\",\n",
    "    tokens_col: str = \"tokens\",\n",
    "    max_len: int = 256,\n",
    "    batch_size: int = 64,\n",
    "    output_label_as: str = \"id\",  # 'id' or 'name'\n",
    "):\n",
    "    \"\"\"\n",
    "    이미 idx 컬럼이 존재하는 submission 템플릿에 class만 채워 넣는 함수.\n",
    "    \"\"\"\n",
    "    # 1. CSV 로드\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    sub_df = pd.read_csv(submission_csv_path)\n",
    "    if \"Unnamed: 0\" in test_df.columns:\n",
    "        test_df = test_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    # 2. 토큰화 확보\n",
    "    if tokens_col in test_df.columns:\n",
    "        maybe = test_df[tokens_col].iloc[0]\n",
    "        if isinstance(maybe, str):\n",
    "            import ast\n",
    "            test_df[tokens_col] = test_df[tokens_col].apply(lambda s: ast.literal_eval(s))\n",
    "    else:\n",
    "        assert preprocess_fn is not None, \"preprocess_fn이 필요합니다.\"\n",
    "        test_df[input_text_col] = test_df[input_text_col].astype(str).fillna(\"\")\n",
    "        test_df[tokens_col] = test_df[input_text_col].apply(lambda s: preprocess_fn(s, stop_words or set()))\n",
    "\n",
    "    # 3. 인코딩\n",
    "    pad_id = stoi[\"<pad>\"]\n",
    "    def _encode_row(tokens):\n",
    "        ids, attn = encode_tokens(tokens, stoi, max_len=max_len, add_cls=True, add_sep=True)\n",
    "        return {\"input_ids\": ids, \"attention_mask\": attn}\n",
    "\n",
    "    encoded = [_encode_row(t) for t in test_df[tokens_col]]\n",
    "    ds = _ListDataset(encoded)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                    collate_fn=lambda b: _collate_batch(b, pad_id))\n",
    "\n",
    "    # 4. 추론\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn = batch[\"attention_mask\"].to(device)\n",
    "            logits = model(input_ids, attention_mask=attn)\n",
    "            preds.extend(logits.argmax(dim=-1).detach().cpu().tolist())\n",
    "\n",
    "    # 5. 결과 매핑\n",
    "    if output_label_as == \"name\":\n",
    "        assert id2label is not None, \"output_label_as='name'이면 id2label 필요.\"\n",
    "        pred_labels = [id2label[i] for i in preds]\n",
    "    else:\n",
    "        pred_labels = preds\n",
    "\n",
    "    # 6. submission 채워넣기\n",
    "    sub_df[\"class\"] = pred_labels\n",
    "    sub_df.to_csv(submission_csv_path, index=False)\n",
    "    print(f\"✅ '{submission_csv_path}' 저장 완료 ({len(sub_df)}개 샘플)\")\n",
    "    return sub_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799102a-6aa3-46a7-98dd-b732bfcfa831",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = fill_submission_class(\n",
    "    test_csv_path=\"../data/test.csv\",\n",
    "    submission_csv_path=\"../data/submission.csv\",\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    encode_tokens=encode_tokens,\n",
    "    preprocess_fn=preprocess_sentence,   # tokens이 이미 있으면 생략 가능\n",
    "    stop_words=stop_words,\n",
    "    id2label=id2label,\n",
    "    input_text_col=\"conversation\",\n",
    "    tokens_col=\"tokens\",\n",
    "    output_label_as=\"id\",   # 'id'면 숫자 라벨, 'name'이면 문자열 라벨\n",
    ")\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f24ea-62e1-46de-9d08-a6544be94165",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cee21531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# ---- small utilities ----\n",
    "class SimpleListDataset(Dataset):\n",
    "    def __init__(self, records):\n",
    "        self.records = records\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.records[idx]\n",
    "\n",
    "def _collect_tokens_rowwise(df, pos_cols):\n",
    "    \"\"\"\n",
    "    From POS columns (each cell = list[str]), flatten into a single tokens list per row.\n",
    "    Assumes columns may be missing or contain NaN.\n",
    "    \"\"\"\n",
    "    vals = df[pos_cols].apply(\n",
    "        lambda row: sum((lst for lst in row if isinstance(lst, (list, tuple))), []),\n",
    "        axis=1\n",
    "    )\n",
    "    return vals\n",
    "\n",
    "def _clean_list(tokens):\n",
    "    return [clean_token(x) for x in tokens if clean_token(x)]\n",
    "\n",
    "def _encode_df(df, stoi, max_len=256):\n",
    "    def _encode_row(row):\n",
    "        ids, attn = encode_tokens(row[\"tokens_sw\"], stoi, max_len=max_len)\n",
    "        return {\"input_ids\": ids, \"attention_mask\": attn, \"label\": label2id[row[\"class\"]]}\n",
    "    return [ _encode_row(r) for _, r in df.iterrows() ]\n",
    "\n",
    "def _make_loaders(train_records, valid_records, pad_id, bs=32):\n",
    "    train_ds = SimpleListDataset(train_records)\n",
    "    valid_ds = SimpleListDataset(valid_records)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=bs, shuffle=True,\n",
    "        collate_fn=lambda b: collate_batch(b, pad_id)\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_ds, batch_size=bs, shuffle=False,\n",
    "        collate_fn=lambda b: collate_batch(b, pad_id)\n",
    "    )\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def _make_optimizer_scheduler(model, lr=3e-4, weight_decay=0.01, warmup_steps=0):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optional simple scheduler (linear warmup -> none): keep compatible with your train_one_epoch signature\n",
    "    scheduler = None\n",
    "    return optimizer, scheduler\n",
    "\n",
    "# ---- main experiment runner ----\n",
    "def run_all_stopword_pos_experiments(\n",
    "    df: pd.DataFrame,\n",
    "    save_csv_path: str | None = \"./experiment_results.csv\",\n",
    "    save_ckpt_dir: str = \"./ckpts\",\n",
    "    batch_size: int = 32,\n",
    "    max_len: int = 256,\n",
    "    lr: float = 3e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs 8 experiment cases:\n",
    "      1) No stopwords,      POS=5 (Noun,Verb,Adjective,Adverb,Exclamation)\n",
    "      2) No stopwords,      POS=3 (Noun,Verb,Adjective)\n",
    "      3) stopwords only,    POS=5\n",
    "      4) stopwords only,    POS=3\n",
    "      5) stopwords|stop_words,              POS=5\n",
    "      6) stopwords|stop_words,              POS=3\n",
    "      7) stopwords|stop_words|top80_new,    POS=5\n",
    "      8) stopwords|stop_words|top80_new,    POS=3\n",
    "\n",
    "    Assumes globals exist: stopwords, stop_words, top80_new_stops, label2id, id2label, epochs,\n",
    "    and helper functions/classes already defined in your script:\n",
    "      - build_vocab, encode_tokens, collate_batch, create_model, clean_token\n",
    "      - train_one_epoch(model, train_loader, optimizer, device, class_weights=..., grad_clip=..., scheduler=..., use_amp=...)\n",
    "      - evaluate(model, valid_loader, device) -> dict with 'f1_macro' (and maybe 'acc')\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # POS sets\n",
    "    POS5 = [\"Noun\", \"Verb\", \"Adjective\", \"Adverb\", \"Exclamation\"]\n",
    "    POS3 = [\"Noun\", \"Verb\", \"Adjective\"]\n",
    "\n",
    "    # stopword combos\n",
    "    sw_none = set()\n",
    "    sw_stop = set(stopwords)\n",
    "    sw_stop_plus = set(stopwords) | set(stop_words)\n",
    "    sw_stop_plus_top80 = sw_stop_plus | set(top80_new_stops)\n",
    "\n",
    "    # define the 8 cases\n",
    "    CASES = [\n",
    "        {\"name\": \"noSW_POS5\",          \"pos_cols\": POS5, \"sw_set\": sw_none,            \"sw_desc\": \"none\"},\n",
    "        {\"name\": \"noSW_POS3\",          \"pos_cols\": POS3, \"sw_set\": sw_none,            \"sw_desc\": \"none\"},\n",
    "        {\"name\": \"stop_only_POS5\",     \"pos_cols\": POS5, \"sw_set\": sw_stop,            \"sw_desc\": \"stopwords\"},\n",
    "        {\"name\": \"stop_only_POS3\",     \"pos_cols\": POS3, \"sw_set\": sw_stop,            \"sw_desc\": \"stopwords\"},\n",
    "        {\"name\": \"stop_stopwords_POS5\",\"pos_cols\": POS5, \"sw_set\": sw_stop_plus,       \"sw_desc\": \"stopwords|stop_words\"},\n",
    "        {\"name\": \"stop_stopwords_POS3\",\"pos_cols\": POS3, \"sw_set\": sw_stop_plus,       \"sw_desc\": \"stopwords|stop_words\"},\n",
    "        {\"name\": \"stop_stop80_POS5\",   \"pos_cols\": POS5, \"sw_set\": sw_stop_plus_top80, \"sw_desc\": \"stopwords|stop_words|top80\"},\n",
    "        {\"name\": \"stop_stop80_POS3\",   \"pos_cols\": POS3, \"sw_set\": sw_stop_plus_top80, \"sw_desc\": \"stopwords|stop_words|top80\"},\n",
    "    ]\n",
    "\n",
    "    # ensure checkpoint dir\n",
    "    import os\n",
    "    os.makedirs(save_ckpt_dir, exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    start_all = time.time()\n",
    "\n",
    "    # split once for fair comparison (same train/valid across cases)\n",
    "    train_df, valid_df = train_test_split(\n",
    "        df, test_size=0.2, random_state=seed, stratify=df[\"class\"]\n",
    "    )\n",
    "\n",
    "    for case in CASES:\n",
    "        t0 = time.time()\n",
    "        case_name = case[\"name\"]\n",
    "        pos_cols = case[\"pos_cols\"]\n",
    "        sw_set   = case[\"sw_set\"]\n",
    "        sw_desc  = case[\"sw_desc\"]\n",
    "\n",
    "        # 1) Compose tokens per row from selected POS\n",
    "        #    We don’t mutate original df; work on copies with new columns.\n",
    "        tr = train_df.copy()\n",
    "        va = valid_df.copy()\n",
    "\n",
    "        tr[\"tokens\"] = _collect_tokens_rowwise(tr, pos_cols).apply(_clean_list)\n",
    "        va[\"tokens\"] = _collect_tokens_rowwise(va, pos_cols).apply(_clean_list)\n",
    "\n",
    "        # 2) Apply stopwords (if any)\n",
    "        if len(sw_set) > 0:\n",
    "            tr[\"tokens_sw\"] = tr[\"tokens\"].apply(lambda toks: [t for t in toks if t not in sw_set])\n",
    "            va[\"tokens_sw\"] = va[\"tokens\"].apply(lambda toks: [t for t in toks if t not in sw_set])\n",
    "            used_sw = f\"|SW={sw_desc}\"\n",
    "        else:\n",
    "            # just copy tokens → tokens_sw\n",
    "            tr[\"tokens_sw\"] = tr[\"tokens\"]\n",
    "            va[\"tokens_sw\"] = va[\"tokens\"]\n",
    "            used_sw = \"|SW=none\"\n",
    "\n",
    "        # 3) Vocab\n",
    "        stoi, itos, counter = build_vocab(tr[\"tokens_sw\"], min_freq=1, max_size=20000)\n",
    "        pad_id = stoi[\"<pad>\"]\n",
    "\n",
    "        # 4) Encode\n",
    "        train_records = _encode_df(tr, stoi, max_len=max_len)\n",
    "        valid_records = _encode_df(va, stoi, max_len=max_len)\n",
    "\n",
    "        # 5) Loaders\n",
    "        train_loader, valid_loader = _make_loaders(train_records, valid_records, pad_id, bs=batch_size)\n",
    "\n",
    "        # 6) Class weights on *train split*\n",
    "        train_labels = [rec[\"label\"] for rec in train_records]\n",
    "        cnt = Counter(train_labels)\n",
    "        weights = torch.tensor([1.0 / max(cnt.get(i, 1), 1) for i in range(len(label2id))], dtype=torch.float)\n",
    "        weights = weights / weights.mean()\n",
    "        class_weights = weights.to(device)\n",
    "\n",
    "        # 7) Model / Optim / Sched\n",
    "        model = create_model(\n",
    "            vocab_size=len(itos),\n",
    "            num_classes=len(label2id),\n",
    "            pad_id=pad_id,\n",
    "            emb_dim=256, nhead=8, num_layers=4, dim_ff=512, max_len=512, dropout=0.1, use_cls_pool=True\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer, scheduler = _make_optimizer_scheduler(model, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # 8) Train loop\n",
    "        best_f1 = -1.0\n",
    "        best_acc = None\n",
    "        best_epoch = -1\n",
    "        best_ckpt_path = None\n",
    "\n",
    "        for ep in range(1, epochs + 1):\n",
    "            tr_log = train_one_epoch(\n",
    "                model, train_loader, optimizer, device,\n",
    "                class_weights=class_weights, grad_clip=1.0, scheduler=scheduler, use_amp=True\n",
    "            )\n",
    "            va_log = evaluate(model, valid_loader, device)  # dict with f1_macro (and maybe 'acc', 'loss', etc.)\n",
    "\n",
    "            f1 = va_log.get(\"f1_macro\", None)\n",
    "            acc = va_log.get(\"acc\", None)\n",
    "\n",
    "            if f1 is not None and f1 > best_f1:\n",
    "                best_f1 = float(f1)\n",
    "                best_acc = float(acc) if acc is not None else None\n",
    "                best_epoch = ep\n",
    "                # save\n",
    "                ckpt_name = f\"{case_name}_F1-{best_f1:.4f}_ep{best_epoch}.pt\"\n",
    "                best_ckpt_path = os.path.join(save_ckpt_dir, ckpt_name)\n",
    "                torch.save(model.state_dict(), best_ckpt_path)\n",
    "                print(f\"[{case_name}] ✔ New best at epoch {ep}: f1_macro={best_f1:.4f} (ckpt saved)\")\n",
    "\n",
    "        # 9) record results\n",
    "        elapsed = time.time() - t0\n",
    "        rows.append({\n",
    "            \"case\": case_name,\n",
    "            \"pos_cols\": \",\".join(pos_cols),\n",
    "            \"stopwords_mode\": sw_desc,\n",
    "            \"vocab_size\": len(itos),\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_f1_macro\": best_f1 if best_f1 is not None else float(\"nan\"),\n",
    "            \"best_acc\": best_acc if best_acc is not None else float(\"nan\"),\n",
    "            \"ckpt_path\": best_ckpt_path,\n",
    "            \"time_sec\": round(elapsed, 2),\n",
    "        })\n",
    "\n",
    "    total_elapsed = time.time() - start_all\n",
    "    print(f\"All experiments finished in {total_elapsed/60:.1f} min\")\n",
    "\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=[\"best_f1_macro\"], ascending=False).reset_index(drop=True)\n",
    "    if save_csv_path:\n",
    "        results_df.to_csv(save_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Results saved to {save_csv_path}\")\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9303bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS5] ✔ New best at epoch 1: f1_macro=0.6989 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS5] ✔ New best at epoch 2: f1_macro=0.7628 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS5] ✔ New best at epoch 3: f1_macro=0.7634 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS5] ✔ New best at epoch 4: f1_macro=0.7924 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS5] ✔ New best at epoch 9: f1_macro=0.8219 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS5] ✔ New best at epoch 18: f1_macro=0.8293 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS3] ✔ New best at epoch 1: f1_macro=0.6752 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS3] ✔ New best at epoch 2: f1_macro=0.7600 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS3] ✔ New best at epoch 3: f1_macro=0.7732 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS3] ✔ New best at epoch 5: f1_macro=0.8006 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS3] ✔ New best at epoch 7: f1_macro=0.8076 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS3] ✔ New best at epoch 10: f1_macro=0.8118 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noSW_POS3] ✔ New best at epoch 12: f1_macro=0.8287 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS5] ✔ New best at epoch 1: f1_macro=0.6785 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS5] ✔ New best at epoch 2: f1_macro=0.7179 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS5] ✔ New best at epoch 3: f1_macro=0.7669 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS5] ✔ New best at epoch 4: f1_macro=0.7947 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS5] ✔ New best at epoch 11: f1_macro=0.7952 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS5] ✔ New best at epoch 13: f1_macro=0.8150 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS5] ✔ New best at epoch 17: f1_macro=0.8247 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS3] ✔ New best at epoch 1: f1_macro=0.6928 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS3] ✔ New best at epoch 2: f1_macro=0.7413 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS3] ✔ New best at epoch 3: f1_macro=0.7646 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS3] ✔ New best at epoch 5: f1_macro=0.7846 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS3] ✔ New best at epoch 7: f1_macro=0.7877 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS3] ✔ New best at epoch 10: f1_macro=0.7906 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS3] ✔ New best at epoch 17: f1_macro=0.8000 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_only_POS3] ✔ New best at epoch 19: f1_macro=0.8114 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS5] ✔ New best at epoch 1: f1_macro=0.6592 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS5] ✔ New best at epoch 2: f1_macro=0.7436 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS5] ✔ New best at epoch 4: f1_macro=0.7750 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS5] ✔ New best at epoch 5: f1_macro=0.7772 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS5] ✔ New best at epoch 6: f1_macro=0.7957 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS5] ✔ New best at epoch 9: f1_macro=0.7960 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS5] ✔ New best at epoch 10: f1_macro=0.8086 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 1: f1_macro=0.7196 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 2: f1_macro=0.7687 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 3: f1_macro=0.7800 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 5: f1_macro=0.8061 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 11: f1_macro=0.8064 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 14: f1_macro=0.8064 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 16: f1_macro=0.8103 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 17: f1_macro=0.8105 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 18: f1_macro=0.8117 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stopwords_POS3] ✔ New best at epoch 19: f1_macro=0.8146 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS5] ✔ New best at epoch 1: f1_macro=0.7286 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS5] ✔ New best at epoch 2: f1_macro=0.7634 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS5] ✔ New best at epoch 4: f1_macro=0.7948 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS5] ✔ New best at epoch 5: f1_macro=0.7961 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS5] ✔ New best at epoch 9: f1_macro=0.8128 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS3] ✔ New best at epoch 1: f1_macro=0.6904 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS3] ✔ New best at epoch 2: f1_macro=0.7422 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS3] ✔ New best at epoch 3: f1_macro=0.7663 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS3] ✔ New best at epoch 4: f1_macro=0.7712 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS3] ✔ New best at epoch 5: f1_macro=0.7957 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS3] ✔ New best at epoch 8: f1_macro=0.7997 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS3] ✔ New best at epoch 10: f1_macro=0.8052 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stop_stop80_POS3] ✔ New best at epoch 19: f1_macro=0.8066 (ckpt saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All experiments finished in 143.5 min\n",
      "Results saved to ./experiment_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>pos_cols</th>\n",
       "      <th>stopwords_mode</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>best_f1_macro</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>ckpt_path</th>\n",
       "      <th>time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noSW_POS5</td>\n",
       "      <td>Noun,Verb,Adjective,Adverb,Exclamation</td>\n",
       "      <td>none</td>\n",
       "      <td>8600</td>\n",
       "      <td>18</td>\n",
       "      <td>0.829331</td>\n",
       "      <td>0.832673</td>\n",
       "      <td>./ckpts\\noSW_POS5_F1-0.8293_ep18.pt</td>\n",
       "      <td>1423.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noSW_POS3</td>\n",
       "      <td>Noun,Verb,Adjective</td>\n",
       "      <td>none</td>\n",
       "      <td>8391</td>\n",
       "      <td>12</td>\n",
       "      <td>0.828749</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>./ckpts\\noSW_POS3_F1-0.8287_ep12.pt</td>\n",
       "      <td>1351.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stop_only_POS5</td>\n",
       "      <td>Noun,Verb,Adjective,Adverb,Exclamation</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>8585</td>\n",
       "      <td>17</td>\n",
       "      <td>0.824681</td>\n",
       "      <td>0.826733</td>\n",
       "      <td>./ckpts\\stop_only_POS5_F1-0.8247_ep17.pt</td>\n",
       "      <td>1105.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stop_stopwords_POS3</td>\n",
       "      <td>Noun,Verb,Adjective</td>\n",
       "      <td>stopwords|stop_words</td>\n",
       "      <td>8361</td>\n",
       "      <td>19</td>\n",
       "      <td>0.814552</td>\n",
       "      <td>0.815842</td>\n",
       "      <td>./ckpts\\stop_stopwords_POS3_F1-0.8146_ep19.pt</td>\n",
       "      <td>1019.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stop_stop80_POS5</td>\n",
       "      <td>Noun,Verb,Adjective,Adverb,Exclamation</td>\n",
       "      <td>stopwords|stop_words|top80</td>\n",
       "      <td>8441</td>\n",
       "      <td>9</td>\n",
       "      <td>0.812804</td>\n",
       "      <td>0.814851</td>\n",
       "      <td>./ckpts\\stop_stop80_POS5_F1-0.8128_ep9.pt</td>\n",
       "      <td>841.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  case                                pos_cols  \\\n",
       "0            noSW_POS5  Noun,Verb,Adjective,Adverb,Exclamation   \n",
       "1            noSW_POS3                     Noun,Verb,Adjective   \n",
       "2       stop_only_POS5  Noun,Verb,Adjective,Adverb,Exclamation   \n",
       "3  stop_stopwords_POS3                     Noun,Verb,Adjective   \n",
       "4     stop_stop80_POS5  Noun,Verb,Adjective,Adverb,Exclamation   \n",
       "\n",
       "               stopwords_mode  vocab_size  best_epoch  best_f1_macro  \\\n",
       "0                        none        8600          18       0.829331   \n",
       "1                        none        8391          12       0.828749   \n",
       "2                   stopwords        8585          17       0.824681   \n",
       "3        stopwords|stop_words        8361          19       0.814552   \n",
       "4  stopwords|stop_words|top80        8441           9       0.812804   \n",
       "\n",
       "   best_acc                                      ckpt_path  time_sec  \n",
       "0  0.832673            ./ckpts\\noSW_POS5_F1-0.8293_ep18.pt   1423.31  \n",
       "1  0.831683            ./ckpts\\noSW_POS3_F1-0.8287_ep12.pt   1351.11  \n",
       "2  0.826733       ./ckpts\\stop_only_POS5_F1-0.8247_ep17.pt   1105.07  \n",
       "3  0.815842  ./ckpts\\stop_stopwords_POS3_F1-0.8146_ep19.pt   1019.20  \n",
       "4  0.814851      ./ckpts\\stop_stop80_POS5_F1-0.8128_ep9.pt    841.61  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 이미 준비된 변수/함수: train_tokens, stopwords, stop_words, top80_new_stops,\n",
    "# label2id, id2label, epochs, build_vocab, encode_tokens, collate_batch, create_model,\n",
    "# clean_token, train_one_epoch, evaluate\n",
    "\n",
    "results_df = run_all_stopword_pos_experiments(\n",
    "    train_tokens,\n",
    "    save_csv_path=\"./experiment_results.csv\",\n",
    "    save_ckpt_dir=\"./ckpts\",\n",
    "    batch_size=32,\n",
    "    max_len=256,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    ")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f6232970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>pos_cols</th>\n",
       "      <th>stopwords_mode</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>best_f1_macro</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>ckpt_path</th>\n",
       "      <th>time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noSW_POS5</td>\n",
       "      <td>Noun,Verb,Adjective,Adverb,Exclamation</td>\n",
       "      <td>none</td>\n",
       "      <td>8600</td>\n",
       "      <td>18</td>\n",
       "      <td>0.829331</td>\n",
       "      <td>0.832673</td>\n",
       "      <td>./ckpts\\noSW_POS5_F1-0.8293_ep18.pt</td>\n",
       "      <td>1423.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noSW_POS3</td>\n",
       "      <td>Noun,Verb,Adjective</td>\n",
       "      <td>none</td>\n",
       "      <td>8391</td>\n",
       "      <td>12</td>\n",
       "      <td>0.828749</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>./ckpts\\noSW_POS3_F1-0.8287_ep12.pt</td>\n",
       "      <td>1351.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stop_only_POS5</td>\n",
       "      <td>Noun,Verb,Adjective,Adverb,Exclamation</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>8585</td>\n",
       "      <td>17</td>\n",
       "      <td>0.824681</td>\n",
       "      <td>0.826733</td>\n",
       "      <td>./ckpts\\stop_only_POS5_F1-0.8247_ep17.pt</td>\n",
       "      <td>1105.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stop_stopwords_POS3</td>\n",
       "      <td>Noun,Verb,Adjective</td>\n",
       "      <td>stopwords|stop_words</td>\n",
       "      <td>8361</td>\n",
       "      <td>19</td>\n",
       "      <td>0.814552</td>\n",
       "      <td>0.815842</td>\n",
       "      <td>./ckpts\\stop_stopwords_POS3_F1-0.8146_ep19.pt</td>\n",
       "      <td>1019.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stop_stop80_POS5</td>\n",
       "      <td>Noun,Verb,Adjective,Adverb,Exclamation</td>\n",
       "      <td>stopwords|stop_words|top80</td>\n",
       "      <td>8441</td>\n",
       "      <td>9</td>\n",
       "      <td>0.812804</td>\n",
       "      <td>0.814851</td>\n",
       "      <td>./ckpts\\stop_stop80_POS5_F1-0.8128_ep9.pt</td>\n",
       "      <td>841.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stop_only_POS3</td>\n",
       "      <td>Noun,Verb,Adjective</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>8376</td>\n",
       "      <td>19</td>\n",
       "      <td>0.811430</td>\n",
       "      <td>0.813861</td>\n",
       "      <td>./ckpts\\stop_only_POS3_F1-0.8114_ep19.pt</td>\n",
       "      <td>1014.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stop_stopwords_POS5</td>\n",
       "      <td>Noun,Verb,Adjective,Adverb,Exclamation</td>\n",
       "      <td>stopwords|stop_words</td>\n",
       "      <td>8569</td>\n",
       "      <td>10</td>\n",
       "      <td>0.808601</td>\n",
       "      <td>0.811881</td>\n",
       "      <td>./ckpts\\stop_stopwords_POS5_F1-0.8086_ep10.pt</td>\n",
       "      <td>1067.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stop_stop80_POS3</td>\n",
       "      <td>Noun,Verb,Adjective</td>\n",
       "      <td>stopwords|stop_words|top80</td>\n",
       "      <td>8237</td>\n",
       "      <td>19</td>\n",
       "      <td>0.806648</td>\n",
       "      <td>0.807921</td>\n",
       "      <td>./ckpts\\stop_stop80_POS3_F1-0.8066_ep19.pt</td>\n",
       "      <td>788.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  case                                pos_cols  \\\n",
       "0            noSW_POS5  Noun,Verb,Adjective,Adverb,Exclamation   \n",
       "1            noSW_POS3                     Noun,Verb,Adjective   \n",
       "2       stop_only_POS5  Noun,Verb,Adjective,Adverb,Exclamation   \n",
       "3  stop_stopwords_POS3                     Noun,Verb,Adjective   \n",
       "4     stop_stop80_POS5  Noun,Verb,Adjective,Adverb,Exclamation   \n",
       "5       stop_only_POS3                     Noun,Verb,Adjective   \n",
       "6  stop_stopwords_POS5  Noun,Verb,Adjective,Adverb,Exclamation   \n",
       "7     stop_stop80_POS3                     Noun,Verb,Adjective   \n",
       "\n",
       "               stopwords_mode  vocab_size  best_epoch  best_f1_macro  \\\n",
       "0                        none        8600          18       0.829331   \n",
       "1                        none        8391          12       0.828749   \n",
       "2                   stopwords        8585          17       0.824681   \n",
       "3        stopwords|stop_words        8361          19       0.814552   \n",
       "4  stopwords|stop_words|top80        8441           9       0.812804   \n",
       "5                   stopwords        8376          19       0.811430   \n",
       "6        stopwords|stop_words        8569          10       0.808601   \n",
       "7  stopwords|stop_words|top80        8237          19       0.806648   \n",
       "\n",
       "   best_acc                                      ckpt_path  time_sec  \n",
       "0  0.832673            ./ckpts\\noSW_POS5_F1-0.8293_ep18.pt   1423.31  \n",
       "1  0.831683            ./ckpts\\noSW_POS3_F1-0.8287_ep12.pt   1351.11  \n",
       "2  0.826733       ./ckpts\\stop_only_POS5_F1-0.8247_ep17.pt   1105.07  \n",
       "3  0.815842  ./ckpts\\stop_stopwords_POS3_F1-0.8146_ep19.pt   1019.20  \n",
       "4  0.814851      ./ckpts\\stop_stop80_POS5_F1-0.8128_ep9.pt    841.61  \n",
       "5  0.813861       ./ckpts\\stop_only_POS3_F1-0.8114_ep19.pt   1014.41  \n",
       "6  0.811881  ./ckpts\\stop_stopwords_POS5_F1-0.8086_ep10.pt   1067.42  \n",
       "7  0.807921     ./ckpts\\stop_stop80_POS3_F1-0.8066_ep19.pt    788.27  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv('./experiment_results.csv')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d240d1",
   "metadata": {},
   "source": [
    "추가로 진행할 내용\n",
    "- morphs 사용해서/ pos 사용 코드 재활용할때 모든 품사 사용하도록 해서 모델 학습 다시 진행\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
